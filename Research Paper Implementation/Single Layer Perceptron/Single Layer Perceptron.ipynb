{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888a635b-115f-45ae-b4f9-01a6a6382919",
   "metadata": {},
   "source": [
    "# Analysis of the predominant phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c260c8a-c0e1-4349-8322-1e21916ee87c",
   "metadata": {},
   "source": [
    "$P_a$ = the expected proportion of A-units activated by a stimulus of a given size<br>\n",
    "$P_c$ = the conditional probability that an A-unit which responds to a given stimulus ($S_1$), will allso respond to another given stimulus ($S_2$)<br><br>\n",
    "It can be shown that as the size of the retina is increased,<br>\n",
    "the number of S-points($N_s$) quickly ceases to be a significant parameter,<br>\n",
    "and the values of $P_a$ and $P_c$ approach the value that they would have for a retina with infinitely many points.<br><br>\n",
    "For a large retina, therefore, the equations are as follows:<br><br>($P_a$ approach)<br>\n",
    "$$\n",
    "P_a = \\sum^{x}_{e=\\theta} \\sum^{min(y, e-\\theta)}_{i=\\theta} P(e,i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(e,i) = \n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "    x \\\\\n",
    "    e\\\\\n",
    "    \\end{pmatrix}\n",
    "R^{e}(1-R)^{x-e} \\times \n",
    "    \\begin{pmatrix}\n",
    "    y \\\\\n",
    "    i\\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "R^{i}(1-R)^{y-i}\n",
    "$$\n",
    "and<br>\n",
    "$R$ = proportion of S-points activated by the stimulus<br>\n",
    "$x$ = number of excitatory connections to each A-unit<br>\n",
    "$y$ = number of inhibitory connections to each A-unit<br>\n",
    "$\\theta$ = threshold of A-units<br>\n",
    "(The quantities $e$ and $i$ are the excitatory and inhibitory components of the excitation received by the A-unit from the stimulus.<br>\n",
    "If the algebraic sum $\\alpha = e+i$ is equal to or greater than $\\theta$, the A-unit is assumed to respond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921ad00f-10f5-485e-be8b-cac512042591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "\n",
    "def calculate_Pa(R,x,y,theta):\n",
    "    def P(e, i):\n",
    "        term1 = comb(x, e) * (R**e) * ((1-R)**(x-e))\n",
    "        term2 = comb(y, i) * (R**i) * ((1-R)**(y-i))\n",
    "        return term1 * term2\n",
    "    Pa = sum(sum(P(e,i) for i in range(max(theta, e - theta), min(y, e - theta)+1)) for e in range(theta, x+1))\n",
    "    return Pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27965c9b-e2c1-4ae0-a3ce-d5727af211c6",
   "metadata": {},
   "source": [
    "($P_{c}$ approach)\n",
    "$$\n",
    "P_{c} = \\frac{1}{P_{a}} \\sum^{x}_{e= \\theta } \\sum^{y}_{i={e-\\theta}} \\sum^{e}_{l_{e}=0} \\sum^{i}_{l_{i}=0} \\sum^{x-e}_{g_{e}=0} \\sum^{y-i}_{g_{i}=0} P(e,i,l_{e}, l_{i}, g_{e}, g_{i})\n",
    "$$\n",
    "$$\n",
    "(e - i - l_{e} + l_{i} + g_{e} - g_i \\ge \\theta)\n",
    "$$\n",
    "$$\n",
    "where\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    &P(e,i,l_{e}, l_{i}, g_{e}, g_{i})\\\\\n",
    "    &=\\binom{x}{e} R^{e}(1 - R)^{x - e} \\\\\n",
    "    &\\times\\binom{y}{i} R^{i}(1 - R)^{y - i} \\\\\n",
    "    &\\times \\binom{e}{l_{e}} L^{l_{e}}(1-L)^{e - l_{e}} \\\\\n",
    "    &\\times \\binom{i}{l_{i}} L^{l_{i}}(1-L)^{i - l_{i}} \\\\\n",
    "    &\\times \\binom{x - e}{g_{e}} G^{g_{e}}(1-G)^{x - e - g_{e}} \\\\\n",
    "    &\\times \\binom{y - i}{g_{i}} G^{g_{i}}(1-G)^{y - i - g_{i}} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "and<br>\n",
    "$L$ = proportion of the S-points illuminated by the first stimulus, $S_1$, which are not illumintated by $S_2$<br>\n",
    "$G$ = proportion of the residual S-set (left over from the first stimulus) which is included in the second stimulus($S_2$)\n",
    "$$$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402514ee-6783-44ff-bfac-c1f9b7cac5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_Pa(R,x,y,theta):\n",
    "    def P(e, i):\n",
    "        term1 = comb(x, e) * (R**e) * ((1-R)**(x-e))\n",
    "        term2 = comb(y, i) * (R**i) * ((1-R)**(y-i))\n",
    "        return term1 * term2\n",
    "    Pa = sum(sum(P(e,i) for i in range(max(theta, e - theta), min(y, e - theta)+1)) for e in range(theta, x+1))\n",
    "    return Pa\n",
    "    \n",
    "def calculate_Pc(x, y, theta, R, L, G, Pa):\n",
    "    def P(e,i,le, li, ge, gi):\n",
    "        term1 = comb(x, e) * (R**e) * ((1 - R)**(x - e))\n",
    "        term2 = comb(y, i) * (R**i) * ((1 - R)**(y - i))\n",
    "        term3 = comb(e, le) * (L**le) * ((1 - L)**(e - le))\n",
    "        term4 = comb(i, li) * (L**li) * ((1 - L)**(i - li))\n",
    "        term5 = comb(x - e, ge) * (G**ge) * ((1 - G)**(x - e - ge))\n",
    "        term6 = comb(y - i, gi) * (G**gi) * ((1 - G)**(y - i - gi))\n",
    "        return term1 * term2 * term3 * term4 * term5 * term6\n",
    "    \n",
    "    Pa = calculate_Pa(R, x, y, theta)\n",
    "    Pc = 0\n",
    "    for e in tqdm(range(theta, x + 1)):\n",
    "        for i in range(e - theta, y + 1):\n",
    "            for le in range(0, e + 1):\n",
    "                for li in range(0, i + 1):\n",
    "                    for ge in range(0, x - e + 1):\n",
    "                        for gi in range(0, y - i + 1):\n",
    "                            if (e - i - le + li + ge - gi) >= theta:\n",
    "                                Pc+= P(e,i,le, li, ge, gi)\n",
    "                                \n",
    "    return Pc/ Pa if Pa != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbbb3f0f-a70f-475c-9268-66a529668413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 40.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8039399712510817"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example numbers\n",
    "\n",
    "x = 10   # number of excitatory connections\n",
    "y = 10   # number of inhibitory connections\n",
    "theta = 3  # threshold\n",
    "R = 0.5  # proportion of S-points activated by the stimulus\n",
    "L = 0.3  # proportion for L\n",
    "G = 0.4  # proportion for G\n",
    "\n",
    "Pa = 0.6  # Example value for Pa, to be computed separately as per your model\n",
    "\n",
    "Pc = calculate_Pc(x, y, theta, R, L, G, Pa)\n",
    "Pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada75bf-e2b7-4f89-9876-0243f95982a5",
   "metadata": {},
   "source": [
    "The minimum value of $P_c$ is equal to \n",
    "$$\n",
    "P_{c_{min}} = (1 - L)^{x}(1 - G)^{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "881992a6-481a-4400-9add-b73c755fbcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pc_min(L, x, G, y):\n",
    "    term1 = (1 - L)**x\n",
    "    term2 = (1 - G)**y\n",
    "    return term1 * term2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e944f3-58ff-4366-84dd-de3a96fd6523",
   "metadata": {},
   "source": [
    "# Mathematical Analysis of Learning in the Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bec8bb-166e-4a47-a16b-13060f092fea",
   "metadata": {},
   "source": [
    "Probability that the perceptron will show a bias towards the \"correct\" response in preference is called $P_r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2c9b9-a1e3-45c9-b2f1-f71dfe5fc947",
   "metadata": {},
   "source": [
    "Probability that the perceptron will give the correct response for the class of stimuli which is represented.<Br>This probability is called $P_g$, the probability of correct generalization.<br>\n",
    "$$P = P(N_{a_{r}} > 0) \\cdot \\phi(Z)$$\n",
    "$$where$$\n",
    "$$P(N_{a_{r}} > 0) = 1 - (1 - P_{a})^{N_{c}}$$\n",
    "<div align = \"center\">\n",
    "    $\\phi (Z)$ = normal curve integral from $-\\infty$ to $Z$\n",
    "</div>\n",
    "$$and$$\n",
    "$$\n",
    "Z=\\frac{c_1n_{s_{r}} + c_{2}}{\\sqrt{{c_3n_{s_{r}}}^2 + c_4n_{s_{r}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff8fe065-578e-4701-8b0a-0ea65747f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_P(Nc, Pa, c1, c2, c3, c4, nsr):\n",
    "    P_Nar_greater_than_0 = 1 - (1 - Pa) ** Nc\n",
    "    Z = (c1 * nsr + c2) / np.sqrt((c3 * nsr) ** 2 + c4 * nsr)\n",
    "    phi_Z = norm.cdf(Z)\n",
    "\n",
    "    P = P_Nar_greater_than_0 * phi_Z\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70080381-edf4-4e56-bbd8-05029c12e9e1",
   "metadata": {},
   "source": [
    "In ideal environment, consisting of randomly placed points of illumination, where there is no attempt to classify stimuli according to intrinsic similarity.<br>\n",
    "Thus, in a typical learning experiment, we might show the perceptron 1,000 stimuli made up of random collections of illuminated retinal points,<br>and we might arbitrarily reinforce $R_{1}$ as the \"correct\" response for the first 500 of these, and $R_{2}$ for the first 500.<br><br>\n",
    "This environment is \"ideal\" only in the sense that we speak of an ideal gas in physics; it is a convenient artifact for purposes of analysis, and does not lead to the best performance from the perceptron.<br>\n",
    "In the ideal environment situation, the constant $c_{1}$ is always equal to zero, so that, in the case of $P_{g}$ (where $c_{2}$ is also zero), the value of $Z$ will be zero, and $P_{g}$ can never be any better than the random expectation of 0.5.<br>\n",
    "The evaluation of $P_{r}$ for these conditions, however, throws some interesting light on the differences between the alpha, beta, and gamma systems.<br><br>\n",
    "First consider the alpha system, which has the simplest dynamics of the three.<br>\n",
    "In this system, whenever an A-unit is active for one unit of time, it gains one unit of value.<br>\n",
    "We will assume an experiment, initially, in which $N_{a_{r}}$ (the number of stimuli associated to each response) is constant for all responses.<br>\n",
    "In this case, for the sum system,<br>\n",
    "\n",
    "\\begin{cases} \n",
    "    c_1 = 0 \\\\\n",
    "    c_2 = (1-P_a)N_e \\\\\n",
    "    c_3 = 2P_aw \\\\\n",
    "    c_4 \\approx 0 \\\\\n",
    "\\end{cases}\n",
    "\n",
    "where $w = $ the fraction of responses connected to each A-unit.<br>If the source-sets are disjunct, $w = 1 / N_{R},$ where $N_{R}$ is the number of responses in the system.<br>\n",
    "For the $\\mu$-system,<br>\n",
    "\n",
    "\\begin{cases} \n",
    "    c_1 = 0 \\\\\n",
    "    c_2 = (1-P_a)N_e \\\\\n",
    "    c_3 = 0 \\\\\n",
    "    c_4 = 2w \\\\\n",
    "\\end{cases}\n",
    "\n",
    "The reduction of $c_2$ to zero gives the $\\mu$-system a definite advantage over the $\\Sigma$-system.<br>\n",
    "<br> if $n_{a_r}$ instead of being fixed, is treated as a random variable, so that the number of stimuli associated to each response is drawn separately from some distribution, then the performance of the $\\alpha$-system is considerably poorer than the above eqquations indicate.<br>\n",
    "Under these conditions, the constants for the $\\mu$-system are\n",
    "\n",
    "\\begin{cases} \n",
    "    c_1 = 0 \\\\\n",
    "    c_2 = 1-P_a \\\\\n",
    "    c_3 = 2{P_{a}}^{2}{q}^2 [\\frac{(wN_{R}-1)^2}{N_{R}-2}] \\\\\n",
    "    c_4 = \\frac{2(1-P_a)N_{R}}{(1-w_c)N_{A}} \\\\\n",
    "\\end{cases}\n",
    "\n",
    "$where$\n",
    "<div align=\"center\">\n",
    "    <p align=\"left\">\n",
    "        $q =$ ratio of $\\sigma_{n_{S_r{r}}}$ to $\\overleftrightarrow{n}_{S_r}$<br>\n",
    "        $N_R = $ number of responses in the system<br>\n",
    "        $N_A = $ number of A-units in the system<br>\n",
    "        $w_c = $ proportion of A-units common to $R_1$ and $R_2$<br>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba2900-3282-4ae4-a824-967e2a7b74e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
