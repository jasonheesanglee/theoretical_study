{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66391665-8d20-4a72-9b43-e84d4c32caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b794b3-dacf-4427-a669-bd91ef4474eb",
   "metadata": {},
   "source": [
    "# 3.1 Encoder and Decoder Stacks\n",
    "Implemented only based on chapter 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a3ac8-da26-446d-b759-7a3c49e5dad6",
   "metadata": {},
   "source": [
    "## 3.1.1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0850ff76-25ac-47f3-b9bc-b4da6e867b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    '''\n",
    "    Composed of 6 stack of identical layers\n",
    "    Each layer has two sub-layers\n",
    "    \n",
    "    First layer : Multi-Head Self-Attention Mechanism\n",
    "    Second layer : Position-wise fully connected Feed-Forward Network.\n",
    "\n",
    "    Residual Connection around each two sub-layer is employed, followed by layer normalization.\n",
    "    The output of each sub-layer is LayerNorm (x+ Sublayer(x)) where Sublayer(x) is the function implemented by the sub-layer itself.\n",
    "\n",
    "    To facilitate these residual connections, all sub-layers in the model (including embedding layer)\n",
    "    produces outputs of dimension d_model = 512\n",
    "    '''\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        self.d_model = d_model\n",
    "        self.embed = Embed(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.multihead = MultiHead(d_model)\n",
    "        self.feedforward = FeedForward(d_model) \n",
    "        self.layer_norm  = LayerNormalization(d_model)\n",
    "\n",
    "    def normalized(self, output): ### Not yet described, described in ???\n",
    "        normalized_output = LayerNormalization().run(output) # will be defined later on.\n",
    "        return normalized_output \n",
    "        \n",
    "    def add_norm(self, input, layer_output):\n",
    "        # in ma_layer, input will be positional_encoded_embedding\n",
    "        # in ff_layer, input will be the output of ma_layer\n",
    "        return self.layer_norm.run(input + layer_output)\n",
    "        \n",
    "    def mh_layer(self, input):\n",
    "        multihead_attentioned = self.multihead.run(input, input, input)\n",
    "        normalized_ouptut = self.add_norm(input, multihead_attentioned)\n",
    "        return normalized_ouptut\n",
    "        \n",
    "    def ff_layer(self, mh_layer_output):\n",
    "        feed_forwarded = self.feedforward.run(mh_layer_output)\n",
    "        layer_norm_ouptut = self.add_norm(mh_layer_output, feed_forwarded)\n",
    "        return layer_norm_ouptut\n",
    "        \n",
    "    def run(self, input_tokens):\n",
    "        embedded_input = self.embed.token_to_embedding(input_tokens)\n",
    "        positional_encoded_embedding = self.positional_encoding.run(embedded_input)\n",
    "        output = positional_encoded_embedding\n",
    "        for _ in range(6):\n",
    "            ma_layer_output = self.mh_layer(output)\n",
    "            ff_layer_output = self.ff_layer(ma_layer_output)\n",
    "            output = ff_layer_output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbf922-dc56-4d3c-a3f8-9812e0cae169",
   "metadata": {},
   "source": [
    "## 3.1.2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "839a6eea-4f3e-4382-bc6c-21d324d7a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:        \n",
    "    '''\n",
    "    Composed of 6 stack of identical layers.\n",
    "    Each layer has three sub-layers.\n",
    "\n",
    "    First layer : Masked Multi-Head Self-Attention Mechanism\n",
    "    Second layer : Multi-Head Self-Attention Mechanism\n",
    "    Third layer : Position-wise fully connected Feed-Forward Network.\n",
    "\n",
    "    Residual Connection around each two sub-layer is employed, followed by layer normalization.\n",
    "    The output of each sub-layer is LayerNorm (x+ Sublayer(x)) where Sublayer(x) is the function implemented by the sub-layer itself.\n",
    "\n",
    "    To facilitate these residual connections, all sub-layers in the model (including embedding layer)\n",
    "    produces outputs of dimension d_model = 512\n",
    "    '''\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        self.d_model = d_model\n",
    "        self.embed = Embed(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.multihead = MultiHead(d_model)\n",
    "        self.feedforward = FeedForward(d_model) \n",
    "        self.layer_norm = LayerNormalization(d_model)\n",
    "        \n",
    "    def add_norm(self, input, layer_output): ### Not yet described, described in ???\n",
    "        # in ma_layer, input will be positional_encoded_embedding\n",
    "        # in ff_layer, input will be the output of ma_layer\n",
    "        return self.layer_norm.run(input + layer_output)\n",
    "        \n",
    "    def masked_mh_layer(self, input, look_ahead_mask):\n",
    "        masked_mh = self.multihead.run(input, input, input, look_ahead_mask)\n",
    "        normalized_output = self.add_norm(input, masked_mh)\n",
    "        return normalized_output\n",
    "    \n",
    "    def mh_layer(self, masked_mh_layer_output, encoder_output):\n",
    "        multihead_attentioned = self.multihead.run(masked_mh_layer_output, encoder_output, encoder_output)\n",
    "        normalized_ouptut = self.add_norm(masked_mh_layer_output, multihead_attentioned)\n",
    "        return normalized_ouptut\n",
    "        \n",
    "    def ff_layer(self, mh_layer_output):\n",
    "        feed_forwarded = self.feedforward.run(mh_layer_output)\n",
    "        normalized_ouptut = self.add_norm(mh_layer_output, feed_forwarded)\n",
    "        return normalized_ouptut\n",
    "\n",
    "    def run(self, input_tokens, encoder_output):\n",
    "        size = len(input_tokens)\n",
    "        look_ahead_mask = create_look_ahead_mask(size)\n",
    "        \n",
    "        embedded_input = self.embed.token_to_embedding(input_tokens)\n",
    "        positional_encoded_embedding = self.positional_encoding.run(embedded_input)\n",
    "        \n",
    "        for _ in range(6):\n",
    "            layer_normed_masked_mh_output = self.masked_mh_layer(positional_encoded_embedding, look_ahead_mask)\n",
    "            mh_layer_output = self.mh_layer(layer_normed_masked_mh_output, encoder_output)\n",
    "            ff_layer_output = self.ff_layer(mh_layer_output)\n",
    "            positional_encoded_embedding = ff_layer_output\n",
    "        return positional_encoded_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551092c-265d-4a66-98fc-3740561cce68",
   "metadata": {},
   "source": [
    "# 3.2 Attention\n",
    "## 3.2.1 Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c894c105-22a1-4c42-89d9-0cbe18c5da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    '''\n",
    "    Input consists of queries and keys of dimension d_k, and values of dimension d_v\n",
    "    compute the dot products of the query with all keys, divide each by $\\sqrt {d_k}$,\n",
    "    and apply a softmax function to obtain the weights on the values.\n",
    "\n",
    "    In practice, we compute the attention function on a set of queries simultaneously, packed together into matrix Q.\n",
    "    The key in matrix K, the value in matrix V.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def softmax(self, matrix):\n",
    "        e_x = np.exp(matrix - np.max(matrix, axis=-1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    def run(self, Q, K, V, mask=None):\n",
    "        matmul_qk = np.matmul(Q, K.T)/np.sqrt(K.shape[1])\n",
    "        if mask is not None: # added this part after 3.6.2\n",
    "            matmul_qk += (mask * -1e9)\n",
    "        softmaxed = self.softmax(matmul_qk)\n",
    "        result = np.matmul(softmaxed, V)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4bb07f-7cea-4636-b6b5-76c019c7ff95",
   "metadata": {},
   "source": [
    "## 3.2.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7536c995-b4d8-46f3-adf7-dfca0cae7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead:\n",
    "    '''\n",
    "    Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
    "    we found it beneficial to linearly project the queries, keys and values h times \n",
    "    with different, learned linear projections to dk, dk and dv dimensions, respectively.\n",
    "    On each of these projected versions of queries, keys and values\n",
    "    we then perform the attention function in parallel, yielding dv-dimensional output values.\n",
    "    \n",
    "    These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
    "    Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
    "    With a single attention head, averaging inhibits this.\n",
    "\n",
    "    Figure 2 : \n",
    "    $ MultiHead(Q, K, V ) = Concat(head_1, ..., head_h)W^{O}$\n",
    "    $where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$ ######### = ScaledDotProductAttention()\n",
    "\n",
    "    Where the projections are parameter matrices \n",
    "    $W_i^Q ∈ R^{d_{model}\\times d_k }, W_i^K ∈ R^{d_{model}\\timesd_k}, W_i^V ∈ R^{d_{model} \\times d_v} and WO ∈ R^{hd_v \\times d_{model}}$.\n",
    "\n",
    "    In this work we employ $h = 8$ parallel attention layers, or heads. \n",
    "    For each of these we use $d_k = d_v = d_{model}/h = 64$.\n",
    "    Due to the reduced dimension of each head, the total computational cost is similar to\n",
    "    that of single-head attention with full dimensionality.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, d_model=512, num_heads=8):\n",
    "        '''\n",
    "        d_model = dimension of input vector\n",
    "        num_heads = number of attention heads to use / h = 8 as per the paper\n",
    "        '''\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        '''\n",
    "        Calculates the dimension of each head.\n",
    "        It divides the dimension of the model by the number of heads.\n",
    "        To ensure the input is evenly split across the heads.\n",
    "        '''\n",
    "        \n",
    "        self.wq = [np.random.randn(d_model, self.depth) for _ in range(num_heads)]\n",
    "        self.wk = [np.random.randn(d_model, self.depth) for _ in range(num_heads)]\n",
    "        self.wv = [np.random.randn(d_model, self.depth) for _ in range(num_heads)]\n",
    "        '''\n",
    "        Initializes the matrix randomly with np.random.randn, generates a sample from a Gaussian distribution.\n",
    "        It repeats for num_heads (8 here) times.\n",
    "        The dimension of this matrix is (d_model, self.depth)\n",
    "        '''\n",
    "        self.wo = np.random.randn(d_model, d_model)\n",
    "        '''\n",
    "        Initializes the WO matrix randomly with np.random.randn, generates a sample from a Gaussian distribution.\n",
    "        '''\n",
    "\n",
    "        self.attention = ScaledDotProductAttention() # Attention\n",
    "\n",
    "    def run(self, Q, K, V, mask=None):\n",
    "        heads = []\n",
    "        for i in range(self.num_heads):\n",
    "            # split and apply attention to each head\n",
    "            dot_Q = np.dot(Q, self.wq[i])\n",
    "            dot_K = np.dot(K, self.wk[i])\n",
    "            dot_V = np.dot(V, self.wv[i])\n",
    "            heads.append(self.attention.run(dot_Q, dot_K, dot_V, mask))\n",
    "\n",
    "        # concatenate and apply final linear layer\n",
    "        concatenated = np.concatenate(heads, axis=-1)\n",
    "        return np.dot(concatenated, self.wo)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db660446-01d7-47ed-9cbd-1c9a2b2f450b",
   "metadata": {},
   "source": [
    "# 3.3 Position-wise Feed-Forward Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3167368c-41b1-4a15-a450-0d44516f0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    '''\n",
    "    In addition to attention sub-layers, each of the layers in our encoder and decoder contains\n",
    "    a fully connected feed-forward network, which is applied to each position separately and identically.\n",
    "    This consists of two linear transformations with a ReLU activation in between.\n",
    "    $FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 $\n",
    "    While the linear transformations are the same across different positions, they use different parameters from layer to layer.\n",
    "    Another way of describing this is as two convolutions with kernel size 1.\n",
    "    The dimensionality of input and output is $d_model = 512$, and the inner-layer has dimensionality$d_{ff} = 2048$.\n",
    "    '''\n",
    "    def __init__(self, d_model=512, d_ff=2048):\n",
    "        # weight\n",
    "        self.W1 = np.random.randn(d_model, d_ff) # transform input vector from d_model to d_ff\n",
    "        self.W2 = np.random.randn(d_ff, d_model) # transform the transformed vector back to the dimension of d_model\n",
    "\n",
    "        # bias \n",
    "        self.b1 = np.zeros(d_ff) # bias_1 = size of d_ff\n",
    "        self.b2 = np.zeros(d_model) # bias_2 = size of d_model\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "        \n",
    "    def run(self, mh_output):\n",
    "        w1_b1 = self.relu(np.dot(mh_output, self.W1) + self.b1) # First linear transformation\n",
    "        output = np.dot(w1_b1, self.W2) + self.b2 # Second linear transformation\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8782802-a5f1-44b7-8d63-6f808e5fe485",
   "metadata": {},
   "source": [
    "# 3.4 Embeddings and Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "93470dd5-b920-45af-a195-9de8255e390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed:\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.weight = np.random.randn(vocab_size, d_model) * (1/np.sqrt(d_model))\n",
    "\n",
    "    def token_to_embedding(self, token_ids):\n",
    "        embeddings = self.weight[token_ids] * np.sqrt(self.d_model)\n",
    "        return embeddings\n",
    "\n",
    "    def output_to_probabilities(self, decoder_output):\n",
    "        logits = decoder_output @ self.weight.T # Linear Transformation\n",
    "        probabilities = self.softmax(logits) # Softmax\n",
    "        return probabilities\n",
    "        \n",
    "    def softmax(self, logits):\n",
    "        e_x = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a8d6ca-7bf3-4c45-b9e2-85199253836d",
   "metadata": {},
   "source": [
    "# 3.5 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bc0e547a-13b5-40f7-aacd-47c2aa039a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__ (self, d_model=512, max_len=512):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def sin_wave(self, pos, i):\n",
    "        return np.sin(pos/10000 ** (2 * i / self.d_model))\n",
    "\n",
    "    def cos_wave(self, pos, i):\n",
    "        return np.cos(pos/10000 ** (2 * i / self.d_model))\n",
    "\n",
    "    def run(self, input):\n",
    "        seq_len = input.shape[0]\n",
    "        pos_encoding = np.zeros((seq_len, self.d_model))\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                pos_encoding[pos, i] = self.sin_wave(pos, i)\n",
    "                if i + 1 < self.d_model:\n",
    "                    pos_encoding[pos, i + 1] = self.cos_wave(pos, i + 1)\n",
    "\n",
    "        # pos_encoding = pos_encoding[:seq_len, :]\n",
    "        return input + pos_encoding#[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a905b-1a3c-4cf0-b775-53c8d52e439f",
   "metadata": {},
   "source": [
    "# 3.6 What can be added to complete the Transformer\n",
    "## 3.6.1 Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f44b5061-a804-41e6-95b1-ac18201ba9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:\n",
    "    def __init__(self, d_model, epsilon=1e-6):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "\n",
    "    def run(self, x):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        variance= np.var(x, axis=-1, keepdims=True)\n",
    "        normalized = (x - mean) / np.sqrt(variance + self.epsilon)\n",
    "        return self.gamma * normalized + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae6762-7401-4b0e-b0f4-999e5615d5bd",
   "metadata": {},
   "source": [
    "## 3.6.2 Mask Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "19d5aa5f-9a0a-4755-9c3e-a1ef7d3f2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask=np.triu(np.ones((size, size)), k=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22395573-1148-41c6-9e39-0cadd093d7c5",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d15cf0-8584-4c18-ab79-703ef8268189",
   "metadata": {},
   "source": [
    "For the token_ids, I have simply used index of each word in the sentences.<br>\n",
    "Tokenizer will be implemented in different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9cf7b2a9-2a71-4361-a1cb-8b28c07a8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, original_text, translated_text, d_model=512):\n",
    "        splitted_ori = original_text.split()\n",
    "        splitted_trans = translated_text.split()\n",
    "        ori_vocab = {}\n",
    "        for idx, word in enumerate(splitted_ori):\n",
    "            if word not in ori_vocab.keys():\n",
    "                ori_vocab[word] = idx\n",
    "        trans_vocab = {}\n",
    "        for idx, word in enumerate(splitted_trans):\n",
    "            if word not in trans_vocab.keys():\n",
    "                trans_vocab[word] = idx\n",
    "        self.ori_token_id = [ori_vocab[word] for word in splitted_ori]\n",
    "        self.trans_token_id = [trans_vocab[word] for word in splitted_trans]\n",
    "        ori_vocab_size = len(splitted_ori)\n",
    "        trans_vocab_size = len(splitted_trans)        \n",
    "        self.encoder = Encoder(vocab_size=ori_vocab_size)\n",
    "        self.decoder = Decoder(vocab_size=trans_vocab_size)\n",
    "        self.embed = Embed(vocab_size=trans_vocab_size, d_model=d_model)\n",
    "\n",
    "    def run(self):\n",
    "        encoder_output = self.encoder.run(self.ori_token_id)\n",
    "        decoder_output = self.decoder.run(self.trans_token_id, encoder_output)\n",
    "        final_embedding = self.embed.output_to_probabilities(decoder_output)\n",
    "        return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7a0e2e23-f05d-4671-a4a9-87c98fce508f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07658122, 0.39176381, 0.23111772, 0.30053725],\n",
       "       [0.07658122, 0.39176381, 0.23111772, 0.30053725],\n",
       "       [0.07658122, 0.39176381, 0.23111772, 0.30053725],\n",
       "       [0.07658122, 0.39176381, 0.23111772, 0.30053725]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = \"I am so hungry\"\n",
    "translated_text = \"나는 너무 배가 고프다\"\n",
    "transformer = Transformer(original_text, translated_text)\n",
    "transformer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b5b6f249-d3b4-4b7d-a469-ebf578490fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16249228, 0.60308414, 0.23442357],\n",
       "       [0.16249228, 0.60308414, 0.23442357],\n",
       "       [0.16249228, 0.60308414, 0.23442357]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = \"I am so hungry\"\n",
    "translated_text = \"tengo mucha hambre\"\n",
    "transformer = Transformer(original_text, translated_text)\n",
    "transformer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c76e649c-af58-409b-a74b-3841cc7cdfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17070206, 0.15619766, 0.67310027],\n",
       "       [0.17070206, 0.15619766, 0.67310027],\n",
       "       [0.17070206, 0.15619766, 0.67310027]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = \"I am so hungry\"\n",
    "translated_text = \"j'ai tres faim\"\n",
    "transformer = Transformer(original_text, translated_text)\n",
    "transformer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60bc1f-abba-4c0c-b46a-fd188bc0f21d",
   "metadata": {},
   "source": [
    "# Original Transformer\n",
    "Found at www.github.com/tensorflow/tensor2tensor/models/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bdb98c-d424-4a19-8574-ec8d42ed7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 The Tensor2Tensor Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Transformer model from \"Attention Is All You Need\".\n",
    "\n",
    "The Transformer model consists of an encoder and a decoder. Both are stacks\n",
    "of self-attention layers followed by feed-forward layers. This model yields\n",
    "good results on a number of problems, especially in NLP and machine translation.\n",
    "\n",
    "See \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762) for the full\n",
    "description of the model and the results obtained with its early version.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from six.moves import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensor2tensor.data_generators import librispeech\n",
    "from tensor2tensor.layers import common_attention\n",
    "from tensor2tensor.layers import common_hparams\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.layers import modalities\n",
    "from tensor2tensor.layers import transformer_layers\n",
    "from tensor2tensor.layers import transformer_memory\n",
    "from tensor2tensor.utils import beam_search\n",
    "from tensor2tensor.utils import expert_utils\n",
    "from tensor2tensor.utils import mlperf_log\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import t2t_model\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1 import estimator as tf_estimator\n",
    "\n",
    "# pylint: disable=g-direct-tensorflow-import\n",
    "from tensorflow.python.ops import inplace_ops\n",
    "from tensorflow.python.util import nest\n",
    "# pylint: enable=g-direct-tensorflow-import\n",
    "\n",
    "# Alias some commonly reused layers, here and elsewhere.\n",
    "transformer_prepare_encoder = transformer_layers.transformer_prepare_encoder\n",
    "transformer_encoder = transformer_layers.transformer_encoder\n",
    "transformer_ffn_layer = transformer_layers.transformer_ffn_layer\n",
    "\n",
    "\n",
    "def transformer_encode(encoder_function, inputs, target_space, hparams,\n",
    "                       attention_weights=None, features=None, losses=None,\n",
    "                       prepare_encoder_fn=None, **kwargs):\n",
    "  \"\"\"Encode transformer inputs.\n",
    "\n",
    "  Args:\n",
    "    encoder_function: the encoder function\n",
    "    inputs: Transformer inputs [batch_size, input_length, 1, hidden_dim] which\n",
    "      will be flattened along the two spatial dimensions.\n",
    "    target_space: scalar, target space ID.\n",
    "    hparams: hyperparameters for model.\n",
    "    attention_weights: weight to store attention to.\n",
    "    features: optionally pass the entire features dictionary as well. This is\n",
    "      needed now for \"packed\" datasets.\n",
    "    losses: optional list onto which to append extra training losses\n",
    "    prepare_encoder_fn: optional, alternative to transformer_prepare_encoder.\n",
    "    **kwargs: additional arguments to pass to encoder_function\n",
    "\n",
    "  Returns:\n",
    "    Tuple of:\n",
    "        encoder_output: Encoder representation.\n",
    "            [batch_size, input_length, hidden_dim]\n",
    "        encoder_decoder_attention_bias: Bias and mask weights for\n",
    "            encoder-decoder attention. [batch_size, input_length]\n",
    "  \"\"\"\n",
    "  inputs = common_layers.flatten4d3d(inputs)\n",
    "\n",
    "  if not prepare_encoder_fn:\n",
    "    prepare_encoder_fn = transformer_prepare_encoder\n",
    "  encoder_input, self_attention_bias, encoder_decoder_attention_bias = (\n",
    "      prepare_encoder_fn(\n",
    "          inputs, target_space, hparams, features=features))\n",
    "\n",
    "  mlperf_log.transformer_print(\n",
    "      key=mlperf_log.MODEL_HP_LAYER_POSTPROCESS_DROPOUT,\n",
    "      value=hparams.layer_prepostprocess_dropout,\n",
    "      hparams=hparams)\n",
    "\n",
    "  encoder_input = tf.nn.dropout(encoder_input,\n",
    "                                1.0 - hparams.layer_prepostprocess_dropout)\n",
    "\n",
    "  attn_bias_for_padding = None\n",
    "  # Otherwise the encoder will just use encoder_self_attention_bias.\n",
    "  if hparams.unidirectional_encoder:\n",
    "    attn_bias_for_padding = encoder_decoder_attention_bias\n",
    "\n",
    "  encoder_output = encoder_function(\n",
    "      encoder_input,\n",
    "      self_attention_bias,\n",
    "      hparams,\n",
    "      nonpadding=features_to_nonpadding(features, \"inputs\"),\n",
    "      save_weights_to=attention_weights,\n",
    "      make_image_summary=not common_layers.is_xla_compiled(),\n",
    "      losses=losses,\n",
    "      attn_bias_for_padding=attn_bias_for_padding,\n",
    "      **kwargs)\n",
    "\n",
    "  return encoder_output, encoder_decoder_attention_bias\n",
    "\n",
    "\n",
    "def transformer_decode(decoder_function,\n",
    "                       decoder_input,\n",
    "                       encoder_output,\n",
    "                       encoder_decoder_attention_bias,\n",
    "                       decoder_self_attention_bias,\n",
    "                       hparams,\n",
    "                       attention_weights=None,\n",
    "                       cache=None,\n",
    "                       decode_loop_step=None,\n",
    "                       nonpadding=None,\n",
    "                       losses=None,\n",
    "                       **kwargs):\n",
    "  \"\"\"Decode Transformer outputs from encoder representation.\n",
    "\n",
    "  Args:\n",
    "    decoder_function: the decoder function\n",
    "    decoder_input: inputs to bottom of the model. [batch_size, decoder_length,\n",
    "      hidden_dim]\n",
    "    encoder_output: Encoder representation. [batch_size, input_length,\n",
    "      hidden_dim]\n",
    "    encoder_decoder_attention_bias: Bias and mask weights for encoder-decoder\n",
    "      attention. [batch_size, input_length]\n",
    "    decoder_self_attention_bias: Bias and mask weights for decoder\n",
    "      self-attention. [batch_size, decoder_length]\n",
    "    hparams: hyperparameters for model.\n",
    "    attention_weights: weight to store attention to.\n",
    "    cache: dict, containing tensors which are the results of previous\n",
    "      attentions, used for fast decoding.\n",
    "    decode_loop_step: An integer, step number of the decoding loop. Only used\n",
    "      for inference on TPU.\n",
    "    nonpadding: optional Tensor with shape [batch_size, decoder_length]\n",
    "    losses: optional list onto which to append extra training losses\n",
    "    **kwargs: additional arguments to pass to decoder_function\n",
    "\n",
    "  Returns:\n",
    "    Final decoder representation. [batch_size, decoder_length, hidden_dim]\n",
    "  \"\"\"\n",
    "  mlperf_log.transformer_print(\n",
    "      key=mlperf_log.MODEL_HP_LAYER_POSTPROCESS_DROPOUT,\n",
    "      value=hparams.layer_prepostprocess_dropout,\n",
    "      hparams=hparams)\n",
    "  decoder_input = tf.nn.dropout(decoder_input,\n",
    "                                1.0 - hparams.layer_prepostprocess_dropout)\n",
    "\n",
    "  decoder_output = decoder_function(\n",
    "      decoder_input,\n",
    "      encoder_output,\n",
    "      decoder_self_attention_bias,\n",
    "      encoder_decoder_attention_bias,\n",
    "      hparams,\n",
    "      cache=cache,\n",
    "      decode_loop_step=decode_loop_step,\n",
    "      nonpadding=nonpadding,\n",
    "      save_weights_to=attention_weights,\n",
    "      losses=losses,\n",
    "      **kwargs)\n",
    "\n",
    "  if (common_layers.is_xla_compiled() and\n",
    "      hparams.mode == tf_estimator.ModeKeys.TRAIN):\n",
    "    # TPU does not react kindly to extra dimensions.\n",
    "    # TODO(noam): remove this once TPU is more forgiving of extra dims.\n",
    "    return decoder_output\n",
    "  else:\n",
    "    # Expand since t2t expects 4d tensors.\n",
    "    return tf.expand_dims(decoder_output, axis=2)\n",
    "\n",
    "\n",
    "@registry.register_model\n",
    "class Transformer(t2t_model.T2TModel):\n",
    "  \"\"\"Attention net.  See file docstring.\"\"\"\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(Transformer, self).__init__(*args, **kwargs)\n",
    "    self.attention_weights = {}  # For visualizing attention heads.\n",
    "    self.recurrent_memory_by_layer = None  # Override to enable recurrent memory\n",
    "    self._encoder_function = transformer_encoder\n",
    "    self._decoder_function = transformer_decoder\n",
    "    self._init_cache_fn = _init_transformer_cache\n",
    "    self._prepare_encoder_fn = transformer_prepare_encoder\n",
    "    self._prepare_decoder_fn = transformer_prepare_decoder\n",
    "\n",
    "  def encode(self, inputs, target_space, hparams, features=None, losses=None):\n",
    "    \"\"\"Encode transformer inputs, see transformer_encode.\"\"\"\n",
    "    return transformer_encode(\n",
    "        self._encoder_function, inputs, target_space, hparams,\n",
    "        attention_weights=self.attention_weights,\n",
    "        features=features, losses=losses,\n",
    "        prepare_encoder_fn=self._prepare_encoder_fn)\n",
    "\n",
    "  def decode(self,\n",
    "             decoder_input,\n",
    "             encoder_output,\n",
    "             encoder_decoder_attention_bias,\n",
    "             decoder_self_attention_bias,\n",
    "             hparams,\n",
    "             cache=None,\n",
    "             decode_loop_step=None,\n",
    "             nonpadding=None,\n",
    "             losses=None,\n",
    "             **kwargs):\n",
    "    \"\"\"Decode Transformer outputs, see transformer_decode.\"\"\"\n",
    "    return transformer_decode(\n",
    "        self._decoder_function, decoder_input, encoder_output,\n",
    "        encoder_decoder_attention_bias, decoder_self_attention_bias,\n",
    "        hparams, attention_weights=self.attention_weights, cache=cache,\n",
    "        decode_loop_step=decode_loop_step, nonpadding=nonpadding, losses=losses,\n",
    "        **kwargs)\n",
    "\n",
    "  def body(self, features):\n",
    "    \"\"\"Transformer main model_fn.\n",
    "\n",
    "    Args:\n",
    "      features: Map of features to the model. Should contain the following:\n",
    "          \"inputs\": Transformer inputs. [batch_size, input_length, 1,\n",
    "            hidden_dim].\n",
    "          \"targets\": Target decoder outputs. [batch_size, decoder_length, 1,\n",
    "            hidden_dim]\n",
    "          \"target_space_id\": A scalar int from data_generators.problem.SpaceID.\n",
    "\n",
    "    Returns:\n",
    "      Final decoder representation. [batch_size, decoder_length, hidden_dim]\n",
    "    \"\"\"\n",
    "    hparams = self._hparams\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    if self.has_input:\n",
    "      inputs = self._prepare_inputs_for_body(features)\n",
    "      target_space = features[\"target_space_id\"]\n",
    "      encoder_output, encoder_decoder_attention_bias = self.encode(\n",
    "          inputs, target_space, hparams, features=features, losses=losses)\n",
    "    else:\n",
    "      encoder_output, encoder_decoder_attention_bias = (None, None)\n",
    "\n",
    "    targets = features[\"targets\"]\n",
    "    targets_shape = common_layers.shape_list(targets)\n",
    "    targets = common_layers.flatten4d3d(targets)\n",
    "    decoder_input, decoder_self_attention_bias = self._prepare_decoder_fn(\n",
    "        targets, hparams, features=features)\n",
    "\n",
    "    # Not all subclasses of Transformer support keyword arguments related to\n",
    "    # recurrent memory, so only pass these arguments if memory is enabled.\n",
    "    decode_kwargs = {}\n",
    "    if self.recurrent_memory_by_layer is not None:\n",
    "      # TODO(kitaev): The chunk_number feature currently has the same shape as\n",
    "      # \"targets\", but this is only for the purposes of sharing sharding code.\n",
    "      # In fact every token within an example must have the same chunk number.\n",
    "      chunk_number_each_token = tf.squeeze(features[\"chunk_number\"], (-1, -2))\n",
    "      chunk_number_each_example = chunk_number_each_token[:, 0]\n",
    "      # Uncomment the code below to verify that tokens within a batch share the\n",
    "      # same chunk number:\n",
    "      # with tf.control_dependencies([\n",
    "      #     tf.assert_equal(chunk_number_each_token,\n",
    "      #                     chunk_number_each_example[:, None])\n",
    "      # ]):\n",
    "      #   chunk_number_each_example = tf.identity(chunk_number_each_example)\n",
    "      decode_kwargs = dict(\n",
    "          recurrent_memory_by_layer=self.recurrent_memory_by_layer,\n",
    "          chunk_number=chunk_number_each_example,\n",
    "          )\n",
    "    decoder_output = self.decode(\n",
    "        decoder_input,\n",
    "        encoder_output,\n",
    "        encoder_decoder_attention_bias,\n",
    "        decoder_self_attention_bias,\n",
    "        hparams,\n",
    "        nonpadding=features_to_nonpadding(features, \"targets\"),\n",
    "        losses=losses,\n",
    "        **decode_kwargs\n",
    "        )\n",
    "    expected_attentions = features.get(\"expected_attentions\")\n",
    "    if expected_attentions is not None:\n",
    "      attention_loss = common_attention.encoder_decoder_attention_loss(\n",
    "          expected_attentions, self.attention_weights,\n",
    "          hparams.expected_attention_loss_type,\n",
    "          hparams.expected_attention_loss_multiplier)\n",
    "      return decoder_output, {\"attention_loss\": attention_loss}\n",
    "\n",
    "    ret = tf.reshape(decoder_output, targets_shape)\n",
    "    if losses:\n",
    "      return ret, {\"extra_loss\": tf.add_n(losses)}\n",
    "    else:\n",
    "      return ret\n",
    "\n",
    "  def _prepare_inputs_for_body(self, features):\n",
    "    \"\"\"Prepare inputs for body.\n",
    "\n",
    "    Args:\n",
    "      features: Map of string to model features. Should contain\n",
    "          \"inputs\": Transformer inputs. [batch_size, input_length, 1,\n",
    "            hidden_dim].\n",
    "\n",
    "    Returns:\n",
    "      Inputs which will be passed to the model. [batch_size, input_length, 1,\n",
    "          hidden_dim]\n",
    "    \"\"\"\n",
    "    return features[\"inputs\"]\n",
    "\n",
    "  def _greedy_infer(self, features, decode_length, use_tpu=False):\n",
    "    \"\"\"Fast version of greedy decoding.\n",
    "\n",
    "    Args:\n",
    "      features: an map of string to `Tensor`\n",
    "      decode_length: an integer.  How many additional timesteps to decode.\n",
    "      use_tpu: A bool. Whether to build the inference graph for TPU.\n",
    "\n",
    "    Returns:\n",
    "      A dict of decoding results {\n",
    "          \"outputs\": integer `Tensor` of decoded ids of shape\n",
    "              [batch_size, <= decode_length] if beam_size == 1 or\n",
    "              [batch_size, top_beams, <= decode_length]\n",
    "          \"scores\": decoding log probs from the beam search,\n",
    "              None if using greedy decoding (beam_size=1)\n",
    "      }\n",
    "\n",
    "    Raises:\n",
    "      NotImplementedError: If there are multiple data shards.\n",
    "    \"\"\"\n",
    "    # For real-valued modalities use the slow decode path for now.\n",
    "    if (self._target_modality_is_real or\n",
    "        self._hparams.self_attention_type != \"dot_product\"):\n",
    "      return super(Transformer, self)._greedy_infer(features, decode_length)\n",
    "    with tf.variable_scope(self.name):\n",
    "      if use_tpu:\n",
    "        return self._fast_decode_tpu(features, decode_length)\n",
    "      return self._fast_decode(features, decode_length)\n",
    "\n",
    "  def _beam_decode(self,\n",
    "                   features,\n",
    "                   decode_length,\n",
    "                   beam_size,\n",
    "                   top_beams,\n",
    "                   alpha,\n",
    "                   use_tpu=False):\n",
    "    \"\"\"Beam search decoding.\n",
    "\n",
    "    Args:\n",
    "      features: an map of string to `Tensor`\n",
    "      decode_length: an integer.  How many additional timesteps to decode.\n",
    "      beam_size: number of beams.\n",
    "      top_beams: an integer. How many of the beams to return.\n",
    "      alpha: Float that controls the length penalty. larger the alpha, stronger\n",
    "        the preference for longer translations.\n",
    "      use_tpu: A bool, whether to do beam decode on TPU.\n",
    "\n",
    "    Returns:\n",
    "      A dict of decoding results {\n",
    "          \"outputs\": integer `Tensor` of decoded ids of shape\n",
    "              [batch_size, <= decode_length] if beam_size == 1 or\n",
    "              [batch_size, top_beams, <= decode_length]\n",
    "          \"scores\": decoding log probs from the beam search,\n",
    "              None if using greedy decoding (beam_size=1)\n",
    "      }\n",
    "    \"\"\"\n",
    "    if (self._hparams.self_attention_type not in [\n",
    "        \"dot_product\", \"dot_product_relative\"\n",
    "    ]):\n",
    "      # Caching is not guaranteed to work with attention types other than\n",
    "      # dot_product and dot_product_relative.\n",
    "      return self._beam_decode_slow(features, decode_length, beam_size,\n",
    "                                    top_beams, alpha, use_tpu)\n",
    "    with tf.variable_scope(self.name):\n",
    "      if use_tpu:\n",
    "        return self._fast_decode_tpu(features, decode_length, beam_size,\n",
    "                                     top_beams, alpha)\n",
    "      return self._fast_decode(features, decode_length, beam_size, top_beams,\n",
    "                               alpha)\n",
    "\n",
    "  def _prepare_inputs_for_decode(self, features):\n",
    "    \"\"\"Prepare inputs for decoding.\n",
    "\n",
    "    Args:\n",
    "      features: A map of string to model features.\n",
    "\n",
    "    Returns:\n",
    "      Inputs after fixing shape and applying modality.\n",
    "    \"\"\"\n",
    "    dp = self._data_parallelism\n",
    "    hparams = self._hparams\n",
    "    inputs = features[\"inputs\"]\n",
    "    # TODO(llion): Clean up this reshaping logic.\n",
    "    inputs = tf.expand_dims(inputs, axis=1)\n",
    "    if len(inputs.shape) < 5:\n",
    "      inputs = tf.expand_dims(inputs, axis=4)\n",
    "    s = common_layers.shape_list(inputs)\n",
    "    inputs = tf.reshape(inputs, [s[0] * s[1], s[2], s[3], s[4]])\n",
    "    # _shard_features called to ensure that the variable names match\n",
    "    inputs = self._shard_features({\"inputs\": inputs})[\"inputs\"]\n",
    "    input_modality = self._problem_hparams.modality[\"inputs\"]\n",
    "    input_vocab_size = self._problem_hparams.vocab_size[\"inputs\"]\n",
    "    if input_vocab_size is not None and hasattr(hparams, \"vocab_divisor\"):\n",
    "      input_vocab_size += (-input_vocab_size) % hparams.vocab_divisor\n",
    "    modality_name = hparams.name.get(\"inputs\",\n",
    "                                     modalities.get_name(input_modality))(\n",
    "                                         hparams, input_vocab_size)\n",
    "    with tf.variable_scope(modality_name):\n",
    "      bottom = hparams.bottom.get(\"inputs\",\n",
    "                                  modalities.get_bottom(input_modality))\n",
    "      inputs = dp(bottom, inputs, hparams, input_vocab_size)\n",
    "    return inputs\n",
    "\n",
    "  def _fast_decode_tpu(self,\n",
    "                       features,\n",
    "                       decode_length,\n",
    "                       beam_size=1,\n",
    "                       top_beams=1,\n",
    "                       alpha=1.0):\n",
    "    \"\"\"Fast decoding.\n",
    "\n",
    "    Implements both greedy and beam search decoding on TPU, uses beam search\n",
    "    iff beam_size > 1, otherwise beam search related arguments are ignored.\n",
    "\n",
    "    Args:\n",
    "      features: A map of string to model features.\n",
    "      decode_length: An integer, how many additional timesteps to decode.\n",
    "      beam_size: An integer, number of beams.\n",
    "      top_beams: An integer, how many of the beams to return.\n",
    "      alpha: A float that controls the length penalty. Larger the alpha,\n",
    "        stronger the preference for longer translations.\n",
    "\n",
    "    Returns:\n",
    "      A dict of decoding results {\n",
    "          \"outputs\": integer `Tensor` of decoded ids of shape\n",
    "              [batch_size, <= decode_length] if beam_size == 1 or\n",
    "              [batch_size, top_beams, <= decode_length]\n",
    "          \"scores\": decoding log probs from the beam search,\n",
    "              None if using greedy decoding (beam_size=1)\n",
    "      }.\n",
    "\n",
    "    Raises:\n",
    "      NotImplementedError: If there are multiple data shards.\n",
    "    \"\"\"\n",
    "    if self._num_datashards != 1:\n",
    "      raise NotImplementedError(\"Fast decoding only supports a single shard.\")\n",
    "    if \"targets_segmentation\" in features:\n",
    "      raise NotImplementedError(\n",
    "          \"Decoding not supported on packed datasets \"\n",
    "          \" If you want to decode from a dataset, use the non-packed version\"\n",
    "          \" of the dataset when decoding.\")\n",
    "    dp = self._data_parallelism\n",
    "    hparams = self._hparams\n",
    "    target_modality = self._problem_hparams.modality[\"targets\"]\n",
    "    target_vocab_size = self._problem_hparams.vocab_size[\"targets\"]\n",
    "    if target_vocab_size is not None and hasattr(hparams, \"vocab_divisor\"):\n",
    "      target_vocab_size += (-target_vocab_size) % hparams.vocab_divisor\n",
    "\n",
    "    if self.has_input:\n",
    "      inputs_shape = common_layers.shape_list(features[\"inputs\"])\n",
    "      if (target_modality == modalities.ModalityType.CLASS_LABEL or\n",
    "          self._problem_hparams.get(\"regression_targets\")):\n",
    "        decode_length = 1\n",
    "      else:\n",
    "        decode_length = (\n",
    "            inputs_shape[1] + features.get(\"decode_length\", decode_length))\n",
    "      batch_size = inputs_shape[0]\n",
    "      inputs = self._prepare_inputs_for_decode(features)\n",
    "      with tf.variable_scope(\"body\"):\n",
    "        encoder_output, encoder_decoder_attention_bias = dp(\n",
    "            self.encode,\n",
    "            inputs,\n",
    "            features[\"target_space_id\"],\n",
    "            hparams,\n",
    "            features=features)\n",
    "      encoder_output = encoder_output[0]\n",
    "      encoder_decoder_attention_bias = encoder_decoder_attention_bias[0]\n",
    "      partial_targets = None\n",
    "    else:\n",
    "      # The problem has no inputs.\n",
    "      encoder_output = None\n",
    "      encoder_decoder_attention_bias = None\n",
    "\n",
    "      # Prepare partial targets.\n",
    "      # In either features[\"inputs\"] or features[\"targets\"].\n",
    "      # We force the outputs to begin with these sequences.\n",
    "      partial_targets = features.get(\"inputs\")\n",
    "      if partial_targets is None:\n",
    "        partial_targets = features[\"targets\"]\n",
    "      assert partial_targets is not None\n",
    "      partial_targets = common_layers.expand_squeeze_to_nd(partial_targets, 2)\n",
    "      partial_targets = tf.to_int64(partial_targets)\n",
    "      partial_targets_shape = common_layers.shape_list(partial_targets)\n",
    "      partial_targets_length = partial_targets_shape[1]\n",
    "      decode_length = (\n",
    "          partial_targets_length + features.get(\"decode_length\", decode_length))\n",
    "      batch_size = partial_targets_shape[0]\n",
    "\n",
    "    if hparams.pos == \"timing\":\n",
    "      positional_encoding = common_attention.get_timing_signal_1d(\n",
    "          decode_length + 1, hparams.hidden_size)\n",
    "    elif hparams.pos == \"timing_from_features\":\n",
    "      positional_encoding = common_attention.add_timing_signals_from_features(\n",
    "          tf.zeros([1, decode_length + 1, hparams.hidden_size]), features,\n",
    "          hparams.position_features)\n",
    "    elif hparams.pos == \"emb\":\n",
    "      positional_encoding = common_attention.add_positional_embedding(\n",
    "          tf.zeros([1, decode_length + 1, hparams.hidden_size]),\n",
    "          hparams.max_length, \"body/targets_positional_embedding\", None)\n",
    "    else:\n",
    "      positional_encoding = None\n",
    "\n",
    "    def preprocess_targets(targets, i):\n",
    "      \"\"\"Performs preprocessing steps on the targets to prepare for the decoder.\n",
    "\n",
    "      This includes:\n",
    "        - Embedding the ids.\n",
    "        - Flattening to 3D tensor.\n",
    "        - Optionally adding timing signals.\n",
    "\n",
    "      Args:\n",
    "        targets: A tensor, inputs ids to the decoder. [batch_size, 1].\n",
    "        i: An integer, Step number of the decoding loop.\n",
    "\n",
    "      Returns:\n",
    "        A tensor, processed targets [batch_size, 1, hidden_dim].\n",
    "      \"\"\"\n",
    "      # _shard_features called to ensure that the variable names match\n",
    "      targets = self._shard_features({\"targets\": targets})[\"targets\"]\n",
    "      modality_name = hparams.name.get(\n",
    "          \"targets\",\n",
    "          modalities.get_name(target_modality))(hparams, target_vocab_size)\n",
    "      with tf.variable_scope(modality_name):\n",
    "        bottom = hparams.bottom.get(\n",
    "            \"targets\", modalities.get_targets_bottom(target_modality))\n",
    "        targets = dp(bottom, targets, hparams, target_vocab_size)[0]\n",
    "      targets = common_layers.flatten4d3d(targets)\n",
    "\n",
    "      # GO embeddings are all zero, this is because transformer_prepare_decoder\n",
    "      # Shifts the targets along by one for the input which pads with zeros.\n",
    "      # If the modality already maps GO to the zero embeddings this is not\n",
    "      # needed.\n",
    "      targets = tf.cond(\n",
    "          tf.equal(i, 0), lambda: tf.zeros_like(targets), lambda: targets)\n",
    "\n",
    "      if positional_encoding is not None:\n",
    "        positional_encoding_shape = positional_encoding.shape.as_list()\n",
    "        targets += tf.slice(\n",
    "            positional_encoding, [0, i, 0],\n",
    "            [positional_encoding_shape[0], 1, positional_encoding_shape[2]])\n",
    "      return targets\n",
    "\n",
    "    decoder_self_attention_bias = (\n",
    "        common_attention.attention_bias_lower_triangle(decode_length))\n",
    "    if hparams.proximity_bias:\n",
    "      decoder_self_attention_bias += common_attention.attention_bias_proximal(\n",
    "          decode_length)\n",
    "\n",
    "    def symbols_to_logits_tpu_fn(ids, i, cache):\n",
    "      \"\"\"Go from ids to logits for next symbol on TPU.\n",
    "\n",
    "      Args:\n",
    "        ids: A tensor, symbol IDs.\n",
    "        i: An integer, step number of the decoding loop. Only used for inference\n",
    "          on TPU.\n",
    "        cache: A dict, containing tensors which are the results of previous\n",
    "          attentions, used for fast decoding.\n",
    "\n",
    "      Returns:\n",
    "        ret: A tensor, computed logits.\n",
    "        cache: A dict, containing tensors which are the results of previous\n",
    "            attentions, used for fast decoding.\n",
    "      \"\"\"\n",
    "      ids = ids[:, -1:]\n",
    "      targets = tf.expand_dims(tf.expand_dims(ids, axis=2), axis=3)\n",
    "      targets = preprocess_targets(targets, i)\n",
    "\n",
    "      bias_shape = decoder_self_attention_bias.shape.as_list()\n",
    "      bias = tf.slice(decoder_self_attention_bias, [0, 0, i, 0],\n",
    "                      [bias_shape[0], bias_shape[1], 1, bias_shape[3]])\n",
    "\n",
    "      with tf.variable_scope(\"body\"):\n",
    "        body_outputs = dp(\n",
    "            self.decode,\n",
    "            targets,\n",
    "            cache.get(\"encoder_output\"),\n",
    "            cache.get(\"encoder_decoder_attention_bias\"),\n",
    "            bias,\n",
    "            hparams,\n",
    "            cache,\n",
    "            i,\n",
    "            nonpadding=features_to_nonpadding(features, \"targets\"))\n",
    "      modality_name = hparams.name.get(\n",
    "          \"targets\",\n",
    "          modalities.get_name(target_modality))(hparams, target_vocab_size)\n",
    "      with tf.variable_scope(modality_name):\n",
    "        top = hparams.top.get(\"targets\",\n",
    "                              modalities.get_top(target_modality))\n",
    "        logits = dp(top, body_outputs, None, hparams, target_vocab_size)[0]\n",
    "\n",
    "      ret = tf.squeeze(logits, axis=[1, 2, 3])\n",
    "      if partial_targets is not None:\n",
    "        # If the position is within the given partial targets, we alter the\n",
    "        # logits to always return those values.\n",
    "        # A faster approach would be to process the partial targets in one\n",
    "        # iteration in order to fill the corresponding parts of the cache.\n",
    "        # This would require broader changes, though.\n",
    "        vocab_size = tf.shape(ret)[1]\n",
    "\n",
    "        def forced_logits():\n",
    "          return tf.one_hot(\n",
    "              tf.tile(\n",
    "                  tf.slice(partial_targets, [0, i],\n",
    "                           [partial_targets.shape.as_list()[0], 1]),\n",
    "                  [beam_size]), vocab_size, 0.0, -1e9)\n",
    "\n",
    "        ret = tf.cond(\n",
    "            tf.less(i, partial_targets_length), forced_logits, lambda: ret)\n",
    "      return ret, cache\n",
    "\n",
    "    eos_id = self.get_decode_end_id() or beam_search.EOS_ID\n",
    "    temperature = features.get(\"sampling_temp\",\n",
    "                               getattr(hparams, \"sampling_temp\", 0.0))\n",
    "    top_k = features.get(\"sampling_keep_top_k\",\n",
    "                         getattr(hparams, \"sampling_keep_top_k\", -1))\n",
    "\n",
    "    ret = fast_decode_tpu(\n",
    "        encoder_output=encoder_output,\n",
    "        encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n",
    "        symbols_to_logits_fn=symbols_to_logits_tpu_fn,\n",
    "        hparams=hparams,\n",
    "        decode_length=decode_length,\n",
    "        vocab_size=target_vocab_size,\n",
    "        init_cache_fn=self._init_cache_fn,\n",
    "        beam_size=beam_size,\n",
    "        top_beams=top_beams,\n",
    "        alpha=alpha,\n",
    "        batch_size=batch_size,\n",
    "        force_decode_length=self._decode_hparams.force_decode_length,\n",
    "        eos_id=eos_id,\n",
    "        sampling_temperature=temperature,\n",
    "        top_k=top_k)\n",
    "    if partial_targets is not None:\n",
    "      if beam_size <= 1 or top_beams <= 1:\n",
    "        ret[\"outputs\"] = ret[\"outputs\"][:, partial_targets_length:]\n",
    "      else:\n",
    "        ret[\"outputs\"] = ret[\"outputs\"][:, :, partial_targets_length:]\n",
    "    return ret\n",
    "\n",
    "  def get_decode_start_id(self):\n",
    "    \"\"\"Returns the id of the first decoder input symbol.\n",
    "\n",
    "    The default case maps None to a vector of 0's for transformer. This method\n",
    "    can be overridden to return a different id by a model wanting to use a\n",
    "    different decoder start symbol. The id returned by this method is used to\n",
    "    index the embedding matrix, and retrieve the vector that will be used as the\n",
    "    first input to the decoder\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "  def get_decode_end_id(self):\n",
    "    \"\"\"Returns the id of the output symbol that terminates decoding.\n",
    "\n",
    "    This method can be overridden by a different model. The id returned by this\n",
    "    method is used to check if the generation is complete during decoding.\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "  def _fast_decode(self,\n",
    "                   features,\n",
    "                   decode_length,\n",
    "                   beam_size=1,\n",
    "                   top_beams=1,\n",
    "                   alpha=1.0,\n",
    "                   preprocess_targets_method=None):\n",
    "    \"\"\"Fast decoding.\n",
    "\n",
    "    Implements both greedy and beam search decoding, uses beam search iff\n",
    "    beam_size > 1, otherwise beam search related arguments are ignored.\n",
    "\n",
    "    Args:\n",
    "      features: a map of string to model  features.\n",
    "      decode_length: an integer.  How many additional timesteps to decode.\n",
    "      beam_size: number of beams.\n",
    "      top_beams: an integer. How many of the beams to return.\n",
    "      alpha: Float that controls the length penalty. larger the alpha, stronger\n",
    "        the preference for longer translations.\n",
    "      preprocess_targets_method: method used to preprocess targets. If None,\n",
    "      uses method \"preprocess_targets\" defined inside this method.\n",
    "\n",
    "    Returns:\n",
    "      A dict of decoding results {\n",
    "          \"outputs\": integer `Tensor` of decoded ids of shape\n",
    "              [batch_size, <= decode_length] if beam_size == 1 or\n",
    "              [batch_size, top_beams, <= decode_length]\n",
    "          \"scores\": decoding log probs from the beam search,\n",
    "              None if using greedy decoding (beam_size=1)\n",
    "      }\n",
    "\n",
    "    Raises:\n",
    "      NotImplementedError: If there are multiple data shards.\n",
    "    \"\"\"\n",
    "    if self._num_datashards != 1:\n",
    "      raise NotImplementedError(\"Fast decoding only supports a single shard.\")\n",
    "    dp = self._data_parallelism\n",
    "    hparams = self._hparams\n",
    "    target_modality = self._problem_hparams.modality[\"targets\"]\n",
    "    target_vocab_size = self._problem_hparams.vocab_size[\"targets\"]\n",
    "    if target_vocab_size is not None and hasattr(hparams, \"vocab_divisor\"):\n",
    "      target_vocab_size += (-target_vocab_size) % hparams.vocab_divisor\n",
    "    if \"targets_segmentation\" in features:\n",
    "      raise NotImplementedError(\n",
    "          \"Decoding not supported on packed datasets \"\n",
    "          \" If you want to decode from a dataset, use the non-packed version\"\n",
    "          \" of the dataset when decoding.\")\n",
    "    if self.has_input:\n",
    "      inputs_shape = common_layers.shape_list(features[\"inputs\"])\n",
    "      if (target_modality == modalities.ModalityType.CLASS_LABEL or\n",
    "          self._problem_hparams.get(\"regression_targets\")):\n",
    "        decode_length = 1\n",
    "      else:\n",
    "        decode_length = (\n",
    "            inputs_shape[1] + features.get(\"decode_length\", decode_length))\n",
    "      batch_size = inputs_shape[0]\n",
    "      inputs = self._prepare_inputs_for_decode(features)\n",
    "      with tf.variable_scope(\"body\"):\n",
    "        encoder_output, encoder_decoder_attention_bias = dp(\n",
    "            self.encode,\n",
    "            inputs,\n",
    "            features[\"target_space_id\"],\n",
    "            hparams,\n",
    "            features=features)\n",
    "      encoder_output = encoder_output[0]\n",
    "      encoder_decoder_attention_bias = encoder_decoder_attention_bias[0]\n",
    "      partial_targets = features.get(\"partial_targets\")\n",
    "    else:\n",
    "      # The problem has no inputs.\n",
    "      encoder_output = None\n",
    "      encoder_decoder_attention_bias = None\n",
    "\n",
    "      # Prepare partial targets.\n",
    "      # In either features[\"inputs\"] or features[\"targets\"].\n",
    "      # We force the outputs to begin with these sequences.\n",
    "      partial_targets = features.get(\"inputs\")\n",
    "      if partial_targets is None:\n",
    "        partial_targets = features[\"targets\"]\n",
    "      assert partial_targets is not None\n",
    "\n",
    "    if partial_targets is not None:\n",
    "      partial_targets = common_layers.expand_squeeze_to_nd(partial_targets, 2)\n",
    "      partial_targets = tf.to_int64(partial_targets)\n",
    "      partial_targets_shape = common_layers.shape_list(partial_targets)\n",
    "      partial_targets_length = partial_targets_shape[1]\n",
    "      decode_length = (\n",
    "          partial_targets_length + features.get(\"decode_length\", decode_length))\n",
    "      batch_size = partial_targets_shape[0]\n",
    "\n",
    "    if hparams.pos == \"timing\":\n",
    "      positional_encoding = common_attention.get_timing_signal_1d(\n",
    "          decode_length + 1, hparams.hidden_size)\n",
    "    elif hparams.pos == \"timing_from_features\":\n",
    "      positional_encoding = common_attention.add_timing_signals_from_features(\n",
    "          tf.zeros([1, decode_length, hparams.hidden_size]), features,\n",
    "          hparams.position_features)\n",
    "    elif hparams.pos == \"emb\":\n",
    "      positional_encoding = common_attention.add_positional_embedding(\n",
    "          tf.zeros([1, decode_length, hparams.hidden_size]), hparams.max_length,\n",
    "          \"body/targets_positional_embedding\", None)\n",
    "    else:\n",
    "      positional_encoding = None\n",
    "\n",
    "    def preprocess_targets(targets, i):\n",
    "      \"\"\"Performs preprocessing steps on the targets to prepare for the decoder.\n",
    "\n",
    "      This includes:\n",
    "        - Embedding the ids.\n",
    "        - Flattening to 3D tensor.\n",
    "        - Optionally adding timing signals.\n",
    "\n",
    "      Args:\n",
    "        targets: inputs ids to the decoder. [batch_size, 1]\n",
    "        i: scalar, Step number of the decoding loop.\n",
    "\n",
    "      Returns:\n",
    "        Processed targets [batch_size, 1, hidden_dim]\n",
    "      \"\"\"\n",
    "      # _shard_features called to ensure that the variable names match\n",
    "      targets = self._shard_features({\"targets\": targets})[\"targets\"]\n",
    "      modality_name = hparams.name.get(\n",
    "          \"targets\",\n",
    "          modalities.get_name(target_modality))(hparams, target_vocab_size)\n",
    "      with tf.variable_scope(modality_name):\n",
    "        bottom = hparams.bottom.get(\n",
    "            \"targets\", modalities.get_targets_bottom(target_modality))\n",
    "        targets = dp(bottom, targets, hparams, target_vocab_size)[0]\n",
    "      targets = common_layers.flatten4d3d(targets)\n",
    "\n",
    "      # GO embeddings are all zero, this is because transformer_prepare_decoder\n",
    "      # Shifts the targets along by one for the input which pads with zeros.\n",
    "      # If the modality already maps GO to the zero embeddings this is not\n",
    "      # needed.\n",
    "      if not self.get_decode_start_id():\n",
    "        targets = tf.cond(\n",
    "            tf.equal(i, 0), lambda: tf.zeros_like(targets), lambda: targets)\n",
    "\n",
    "      if positional_encoding is not None:\n",
    "        targets += positional_encoding[:, i:i + 1]\n",
    "      return targets\n",
    "\n",
    "    decoder_self_attention_bias = (\n",
    "        common_attention.attention_bias_lower_triangle(decode_length))\n",
    "    if hparams.proximity_bias:\n",
    "      decoder_self_attention_bias += common_attention.attention_bias_proximal(\n",
    "          decode_length)\n",
    "\n",
    "    # Create tensors for encoder-decoder attention history\n",
    "    att_cache = {\"attention_history\": {}}\n",
    "    num_layers = hparams.num_decoder_layers or hparams.num_hidden_layers\n",
    "    if encoder_output is not None:\n",
    "      att_batch_size, enc_seq_length = common_layers.shape_list(\n",
    "          encoder_output)[0:2]\n",
    "      for layer in range(num_layers):\n",
    "        att_cache[\"attention_history\"][\"layer_%d\" % layer] = tf.zeros(\n",
    "            [att_batch_size, hparams.num_heads, 0, enc_seq_length])\n",
    "\n",
    "    def update_decoder_attention_history(cache):\n",
    "      \"\"\"Save attention weights in cache, e.g., for vizualization.\"\"\"\n",
    "      for k in [x for x in self.attention_weights\n",
    "                if \"decoder\" in x and \"self\" not in x and \"logits\" not in x]:\n",
    "        idx = k.find(\"layer_\")\n",
    "        if idx < 0:\n",
    "          continue\n",
    "        # Get layer number from the string name.\n",
    "        layer_nbr = k[idx + 6:]\n",
    "        idx = 0\n",
    "        while idx + 1 < len(layer_nbr) and layer_nbr[:idx + 1].isdigit():\n",
    "          idx += 1\n",
    "        layer_nbr = \"layer_%d\" % int(layer_nbr[:idx])\n",
    "        if layer_nbr in cache[\"attention_history\"]:\n",
    "          cache[\"attention_history\"][layer_nbr] = tf.concat(\n",
    "              [cache[\"attention_history\"][layer_nbr],\n",
    "               self.attention_weights[k]],\n",
    "              axis=2)\n",
    "    if not preprocess_targets_method:\n",
    "      preprocess_targets_method = preprocess_targets\n",
    "\n",
    "    def symbols_to_logits_fn(ids, i, cache):\n",
    "      \"\"\"Go from ids to logits for next symbol.\"\"\"\n",
    "      ids = ids[:, -1:]\n",
    "      targets = tf.expand_dims(tf.expand_dims(ids, axis=2), axis=3)\n",
    "      targets = preprocess_targets_method(targets, i)\n",
    "\n",
    "      bias = decoder_self_attention_bias[:, :, i:i + 1, :i + 1]\n",
    "      with tf.variable_scope(\"body\"):\n",
    "        body_outputs = dp(\n",
    "            self.decode,\n",
    "            targets,\n",
    "            cache.get(\"encoder_output\"),\n",
    "            cache.get(\"encoder_decoder_attention_bias\"),\n",
    "            bias,\n",
    "            hparams,\n",
    "            cache,\n",
    "            nonpadding=features_to_nonpadding(features, \"targets\"))\n",
    "\n",
    "      update_decoder_attention_history(cache)\n",
    "\n",
    "      modality_name = hparams.name.get(\n",
    "          \"targets\",\n",
    "          modalities.get_name(target_modality))(hparams, target_vocab_size)\n",
    "      with tf.variable_scope(modality_name):\n",
    "        top = hparams.top.get(\"targets\", modalities.get_top(target_modality))\n",
    "        logits = dp(top, body_outputs, None, hparams, target_vocab_size)[0]\n",
    "\n",
    "      ret = tf.squeeze(logits, axis=[1, 2, 3])\n",
    "      if partial_targets is not None:\n",
    "        # If the position is within the given partial targets, we alter the\n",
    "        # logits to always return those values.\n",
    "        # A faster approach would be to process the partial targets in one\n",
    "        # iteration in order to fill the corresponding parts of the cache.\n",
    "        # This would require broader changes, though.\n",
    "        vocab_size = tf.shape(ret)[1]\n",
    "\n",
    "        def forced_logits():\n",
    "          return tf.one_hot(\n",
    "              tf.tile(partial_targets[:, i], [beam_size]), vocab_size, 0.0,\n",
    "              -1e9)\n",
    "\n",
    "        ret = tf.cond(\n",
    "            tf.less(i, partial_targets_length), forced_logits, lambda: ret)\n",
    "      return ret, cache\n",
    "\n",
    "    sos_id = self.get_decode_start_id() or 0\n",
    "    eos_id = self.get_decode_end_id() or beam_search.EOS_ID\n",
    "    temperature = features.get(\"sampling_temp\",\n",
    "                               getattr(hparams, \"sampling_temp\", 0.0))\n",
    "    top_k = features.get(\"sampling_keep_top_k\",\n",
    "                         getattr(hparams, \"sampling_keep_top_k\", -1))\n",
    "\n",
    "    ret = fast_decode(\n",
    "        encoder_output=encoder_output,\n",
    "        encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n",
    "        symbols_to_logits_fn=symbols_to_logits_fn,\n",
    "        hparams=hparams,\n",
    "        decode_length=decode_length,\n",
    "        vocab_size=target_vocab_size,\n",
    "        init_cache_fn=self._init_cache_fn,\n",
    "        beam_size=beam_size,\n",
    "        top_beams=top_beams,\n",
    "        alpha=alpha,\n",
    "        batch_size=batch_size,\n",
    "        force_decode_length=self._decode_hparams.force_decode_length,\n",
    "        sos_id=sos_id,\n",
    "        eos_id=eos_id,\n",
    "        sampling_temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        cache=att_cache)\n",
    "    if partial_targets is not None:\n",
    "      if beam_size <= 1 or top_beams <= 1:\n",
    "        ret[\"outputs\"] = ret[\"outputs\"][:, partial_targets_length:]\n",
    "      else:\n",
    "        ret[\"outputs\"] = ret[\"outputs\"][:, :, partial_targets_length:]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _init_transformer_cache(cache, hparams, batch_size, attention_init_length,\n",
    "                            encoder_output, encoder_decoder_attention_bias,\n",
    "                            scope_prefix):\n",
    "  \"\"\"Create the initial cache for Transformer fast decoding.\"\"\"\n",
    "  key_channels = hparams.attention_key_channels or hparams.hidden_size\n",
    "  value_channels = hparams.attention_value_channels or hparams.hidden_size\n",
    "  num_layers = hparams.num_decoder_layers or hparams.num_hidden_layers\n",
    "  vars_3d_num_heads = (\n",
    "      hparams.num_heads if hparams.get(\"attention_variables_3d\") else 0)\n",
    "\n",
    "  if cache is None:\n",
    "    cache = {}\n",
    "  cache.update({\n",
    "      \"layer_%d\" % layer: {  # pylint: disable=g-complex-comprehension\n",
    "          \"k\":\n",
    "              common_attention.split_heads(\n",
    "                  tf.zeros([batch_size,\n",
    "                            attention_init_length,\n",
    "                            key_channels]), hparams.num_heads),\n",
    "          \"v\":\n",
    "              common_attention.split_heads(\n",
    "                  tf.zeros([batch_size,\n",
    "                            attention_init_length,\n",
    "                            value_channels]), hparams.num_heads),\n",
    "      } for layer in range(num_layers)\n",
    "  })\n",
    "\n",
    "  # If `ffn_layer` is in `[\"dense_relu_dense\" or \"conv_hidden_relu\"]`, then the\n",
    "  # cache key \"f\" won't be used, which means that the` shape of cache[\"f\"]`\n",
    "  # won't be changed to\n",
    "  # `[beamsize*batch_size, decode_length, hparams.hidden_size]` and may cause\n",
    "  # error when applying `nest.map reshape function` on it.\n",
    "  if hparams.ffn_layer not in [\"dense_relu_dense\", \"conv_hidden_relu\"]:\n",
    "    for layer in range(num_layers):\n",
    "      cache[\"layer_%d\" % layer][\"f\"] = tf.zeros(\n",
    "          [batch_size, 0, hparams.hidden_size])\n",
    "\n",
    "  if encoder_output is not None:\n",
    "    for layer in range(num_layers):\n",
    "      layer_name = \"layer_%d\" % layer\n",
    "      with tf.variable_scope(\n",
    "          \"%sdecoder/%s/encdec_attention/multihead_attention\" %\n",
    "          (scope_prefix, layer_name)):\n",
    "        k_encdec = common_attention.compute_attention_component(\n",
    "            encoder_output,\n",
    "            key_channels,\n",
    "            name=\"k\",\n",
    "            vars_3d_num_heads=vars_3d_num_heads)\n",
    "        k_encdec = common_attention.split_heads(k_encdec, hparams.num_heads)\n",
    "        v_encdec = common_attention.compute_attention_component(\n",
    "            encoder_output,\n",
    "            value_channels,\n",
    "            name=\"v\",\n",
    "            vars_3d_num_heads=vars_3d_num_heads)\n",
    "        v_encdec = common_attention.split_heads(v_encdec, hparams.num_heads)\n",
    "      cache[layer_name][\"k_encdec\"] = k_encdec\n",
    "      cache[layer_name][\"v_encdec\"] = v_encdec\n",
    "\n",
    "    cache[\"encoder_output\"] = encoder_output\n",
    "    cache[\"encoder_decoder_attention_bias\"] = encoder_decoder_attention_bias\n",
    "  return cache\n",
    "\n",
    "\n",
    "def fast_decode_tpu(encoder_output,\n",
    "                    encoder_decoder_attention_bias,\n",
    "                    symbols_to_logits_fn,\n",
    "                    hparams,\n",
    "                    decode_length,\n",
    "                    vocab_size,\n",
    "                    init_cache_fn=_init_transformer_cache,\n",
    "                    beam_size=1,\n",
    "                    top_beams=1,\n",
    "                    alpha=1.0,\n",
    "                    sos_id=0,\n",
    "                    eos_id=beam_search.EOS_ID,\n",
    "                    batch_size=None,\n",
    "                    force_decode_length=False,\n",
    "                    scope_prefix=\"body/\",\n",
    "                    use_top_k_with_unique=True,\n",
    "                    sampling_temperature=0.0,\n",
    "                    top_k=-1):\n",
    "  \"\"\"Given encoder output and a symbols to logits function, does fast decoding.\n",
    "\n",
    "  Implements both greedy and beam search decoding for TPU, uses beam search iff\n",
    "  beam_size > 1, otherwise beam search related arguments are ignored.\n",
    "\n",
    "  Args:\n",
    "    encoder_output: A tensor, output from encoder.\n",
    "    encoder_decoder_attention_bias: A tensor, bias for use in encoder-decoder\n",
    "      attention.\n",
    "    symbols_to_logits_fn: Incremental decoding, function mapping triple `(ids,\n",
    "      step, cache)` to symbol logits.\n",
    "    hparams: Run hyperparameters.\n",
    "    decode_length: An integer, how many additional timesteps to decode.\n",
    "    vocab_size: Output vocabulary size.\n",
    "    init_cache_fn: Function that returns the initial cache dict.\n",
    "    beam_size: An integer, number of beams.\n",
    "    top_beams: An integer, how many of the beams to return.\n",
    "    alpha: A float that controls the length penalty. Larger the alpha, stronger\n",
    "      the preference for longer translations.\n",
    "    sos_id: Start-of-sequence symbol.\n",
    "    eos_id: End-of-sequence symbol.\n",
    "    batch_size: An integer, must be passed if there is no input.\n",
    "    force_decode_length: A bool, whether to force the full decode length, or if\n",
    "      False, stop when all beams hit eos_id.\n",
    "    scope_prefix: str, prefix for decoder layer variable scopes.\n",
    "    use_top_k_with_unique: bool, whether to use a fast (but decreased precision)\n",
    "      top_k during beam search.\n",
    "    sampling_temperature: scalar, temperature with which to sample.\n",
    "    top_k: scalar, sample only top k.\n",
    "\n",
    "  Returns:\n",
    "    A dict of decoding results {\n",
    "        \"outputs\": integer `Tensor` of decoded ids of shape\n",
    "            [batch_size, <= decode_length] if top_beams == 1 or\n",
    "            [batch_size, top_beams, <= decode_length] otherwise\n",
    "        \"scores\": decoding log probs from the beam search,\n",
    "            None if using greedy decoding (beam_size=1)\n",
    "    }.\n",
    "\n",
    "  Raises:\n",
    "    NotImplementedError: If beam size > 1 with partial targets.\n",
    "  \"\"\"\n",
    "  if encoder_output is not None:\n",
    "    batch_size = common_layers.shape_list(encoder_output)[0]\n",
    "\n",
    "  cache = init_cache_fn(None, hparams, batch_size, decode_length,\n",
    "                        encoder_output, encoder_decoder_attention_bias,\n",
    "                        scope_prefix)\n",
    "\n",
    "  mlperf_log.transformer_print(\n",
    "      key=mlperf_log.MODEL_HP_SEQ_BEAM_SEARCH,\n",
    "      value={\n",
    "          \"vocab_size\": vocab_size,\n",
    "          \"batch_size\": batch_size,\n",
    "          \"beam_size\": beam_size,\n",
    "          \"alpha\": alpha,\n",
    "          \"max_decode_length\": decode_length\n",
    "      },\n",
    "      hparams=hparams)\n",
    "  if beam_size > 1:  # Beam Search\n",
    "    initial_ids = sos_id * tf.ones([batch_size], dtype=tf.int32)\n",
    "    decoded_ids, scores, _ = beam_search.beam_search(\n",
    "        symbols_to_logits_fn,\n",
    "        initial_ids,\n",
    "        beam_size,\n",
    "        decode_length,\n",
    "        vocab_size,\n",
    "        alpha,\n",
    "        states=cache,\n",
    "        eos_id=eos_id,\n",
    "        stop_early=(top_beams == 1),\n",
    "        use_tpu=True,\n",
    "        use_top_k_with_unique=use_top_k_with_unique)\n",
    "\n",
    "    if top_beams == 1:\n",
    "      decoded_ids = decoded_ids[:, 0, 1:]\n",
    "      scores = scores[:, 0]\n",
    "    else:\n",
    "      decoded_ids = decoded_ids[:, :top_beams, 1:]\n",
    "      scores = scores[:, :top_beams]\n",
    "  else:  # Greedy\n",
    "\n",
    "    def inner_loop(i, hit_eos, next_id, decoded_ids, cache, log_prob):\n",
    "      \"\"\"One step of greedy decoding.\"\"\"\n",
    "      logits, cache = symbols_to_logits_fn(next_id, i, cache)\n",
    "      log_probs = common_layers.log_prob_from_logits(logits)\n",
    "      temperature = sampling_temperature\n",
    "      if hparams.sampling_method == \"random_per_example\":\n",
    "        next_id = common_layers.sample_temperature_per_example(\n",
    "            logits, temperature, top_k)\n",
    "      else:\n",
    "        if hparams.sampling_method == \"argmax\":\n",
    "          temperature = 0.0\n",
    "        next_id = common_layers.sample_with_temperature(logits, temperature,\n",
    "                                                        top_k)\n",
    "\n",
    "      log_prob_indices = tf.stack([tf.range(tf.to_int64(batch_size)), next_id],\n",
    "                                  axis=1)\n",
    "      log_prob += tf.gather_nd(\n",
    "          log_probs, log_prob_indices) * (1 - tf.to_float(hit_eos))\n",
    "      # Note(thangluong): we purposely update hit_eos after aggregating log_prob\n",
    "      # There is a subtle detail here that we want to include log_probs up to\n",
    "      # (and inclusive of) the first eos generated, but not subsequent tokens.\n",
    "      hit_eos |= tf.equal(next_id, eos_id)\n",
    "\n",
    "      next_id = tf.expand_dims(next_id, axis=1)\n",
    "      decoded_ids = tf.transpose(decoded_ids)\n",
    "      decoded_ids = inplace_ops.alias_inplace_update(\n",
    "          decoded_ids, i, tf.squeeze(next_id, axis=1))\n",
    "      decoded_ids = tf.transpose(decoded_ids)\n",
    "      return i + 1, hit_eos, next_id, decoded_ids, cache, log_prob\n",
    "\n",
    "    def is_not_finished(i, hit_eos, *_):\n",
    "      finished = i >= decode_length\n",
    "      if not force_decode_length:\n",
    "        finished |= tf.reduce_all(hit_eos)\n",
    "      return tf.logical_not(finished)\n",
    "\n",
    "    decoded_ids = tf.zeros([batch_size, decode_length], dtype=tf.int64)\n",
    "    hit_eos = tf.fill([batch_size], False)\n",
    "    next_id = sos_id * tf.ones([batch_size, 1], dtype=tf.int64)\n",
    "    initial_log_prob = tf.zeros([batch_size], dtype=tf.float32)\n",
    "\n",
    "    def compute_cache_shape_invariants(tensor):\n",
    "      return tf.TensorShape(tensor.shape.as_list())\n",
    "\n",
    "    _, _, _, decoded_ids, _, log_prob = tf.while_loop(\n",
    "        is_not_finished,\n",
    "        inner_loop, [\n",
    "            tf.constant(0), hit_eos, next_id, decoded_ids, cache,\n",
    "            initial_log_prob\n",
    "        ],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape([]),\n",
    "            tf.TensorShape([batch_size]),\n",
    "            tf.TensorShape([batch_size, 1]),\n",
    "            tf.TensorShape([batch_size, decode_length]),\n",
    "            nest.map_structure(compute_cache_shape_invariants, cache),\n",
    "            tf.TensorShape([batch_size]),\n",
    "        ])\n",
    "    scores = log_prob\n",
    "\n",
    "  return {\"outputs\": decoded_ids, \"scores\": scores}\n",
    "\n",
    "\n",
    "def fast_decode(encoder_output,\n",
    "                encoder_decoder_attention_bias,\n",
    "                symbols_to_logits_fn,\n",
    "                hparams,\n",
    "                decode_length,\n",
    "                vocab_size,\n",
    "                init_cache_fn=_init_transformer_cache,\n",
    "                beam_size=1,\n",
    "                top_beams=1,\n",
    "                alpha=1.0,\n",
    "                sos_id=0,\n",
    "                eos_id=beam_search.EOS_ID,\n",
    "                batch_size=None,\n",
    "                force_decode_length=False,\n",
    "                scope_prefix=\"body/\",\n",
    "                sampling_temperature=0.0,\n",
    "                top_k=-1,\n",
    "                cache=None):\n",
    "  \"\"\"Given encoder output and a symbols to logits function, does fast decoding.\n",
    "\n",
    "  Implements both greedy and beam search decoding, uses beam search iff\n",
    "  beam_size > 1, otherwise beam search related arguments are ignored.\n",
    "\n",
    "  Args:\n",
    "    encoder_output: Output from encoder.\n",
    "    encoder_decoder_attention_bias: a bias tensor for use in encoder-decoder\n",
    "      attention\n",
    "    symbols_to_logits_fn: Incremental decoding; function mapping triple `(ids,\n",
    "      step, cache)` to symbol logits.\n",
    "    hparams: run hyperparameters\n",
    "    decode_length: an integer.  How many additional timesteps to decode.\n",
    "    vocab_size: Output vocabulary size.\n",
    "    init_cache_fn: Function that returns the initial cache dict.\n",
    "    beam_size: number of beams.\n",
    "    top_beams: an integer. How many of the beams to return.\n",
    "    alpha: Float that controls the length penalty. larger the alpha, stronger\n",
    "      the preference for longer translations.\n",
    "    sos_id: End-of-sequence symbol in beam search.\n",
    "    eos_id: End-of-sequence symbol in beam search.\n",
    "    batch_size: an integer scalar - must be passed if there is no input\n",
    "    force_decode_length: bool, whether to force the full decode length, or if\n",
    "      False, stop when all beams hit eos_id.\n",
    "    scope_prefix: str, prefix for decoder layer variable scopes.\n",
    "    sampling_temperature: scalar, temperature with which to sample.\n",
    "    top_k: scalar, sample only top k.\n",
    "    cache: cache dictionary for additional predictions.\n",
    "\n",
    "  Returns:\n",
    "      A dict of decoding results {\n",
    "          \"outputs\": integer `Tensor` of decoded ids of shape\n",
    "              [batch_size, <= decode_length] if top_beams == 1 or\n",
    "              [batch_size, top_beams, <= decode_length] otherwise\n",
    "          \"scores\": decoding log probs from the beam search,\n",
    "              None if using greedy decoding (beam_size=1)\n",
    "      }\n",
    "  \"\"\"\n",
    "  if encoder_output is not None:\n",
    "    batch_size = common_layers.shape_list(encoder_output)[0]\n",
    "\n",
    "  cache = init_cache_fn(\n",
    "      cache=cache,\n",
    "      hparams=hparams,\n",
    "      batch_size=batch_size,\n",
    "      attention_init_length=0,\n",
    "      encoder_output=encoder_output,\n",
    "      encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n",
    "      scope_prefix=scope_prefix)\n",
    "\n",
    "  if beam_size > 1:  # Beam Search\n",
    "    initial_ids = sos_id * tf.ones([batch_size], dtype=tf.int32)\n",
    "    decoded_ids, scores, cache = beam_search.beam_search(\n",
    "        symbols_to_logits_fn,\n",
    "        initial_ids,\n",
    "        beam_size,\n",
    "        decode_length,\n",
    "        vocab_size,\n",
    "        alpha,\n",
    "        states=cache,\n",
    "        eos_id=eos_id,\n",
    "        stop_early=(top_beams == 1))\n",
    "\n",
    "    if top_beams == 1:\n",
    "      decoded_ids = decoded_ids[:, 0, 1:]\n",
    "      scores = scores[:, 0]\n",
    "    else:\n",
    "      decoded_ids = decoded_ids[:, :top_beams, 1:]\n",
    "      scores = scores[:, :top_beams]\n",
    "  else:  # Greedy\n",
    "\n",
    "    def inner_loop(i, hit_eos, next_id, decoded_ids, cache, log_prob):\n",
    "      \"\"\"One step of greedy decoding.\"\"\"\n",
    "      logits, cache = symbols_to_logits_fn(next_id, i, cache)\n",
    "      log_probs = common_layers.log_prob_from_logits(logits)\n",
    "      temperature = sampling_temperature\n",
    "      if hparams.sampling_method == \"random_per_example\":\n",
    "        next_id = common_layers.sample_temperature_per_example(\n",
    "            logits, temperature, top_k)\n",
    "      else:\n",
    "        if hparams.sampling_method == \"argmax\":\n",
    "          temperature = 0.0\n",
    "        next_id = common_layers.sample_with_temperature(logits, temperature,\n",
    "                                                        top_k)\n",
    "\n",
    "      log_prob_indices = tf.stack([tf.range(tf.to_int64(batch_size)), next_id],\n",
    "                                  axis=1)\n",
    "      log_prob += tf.gather_nd(\n",
    "          log_probs, log_prob_indices) * (1 - tf.to_float(hit_eos))\n",
    "      # Note(thangluong): we purposely update hit_eos after aggregating log_prob\n",
    "      # There is a subtle detail here that we want to include log_probs up to\n",
    "      # (and inclusive of) the first eos generated, but not subsequent tokens.\n",
    "      hit_eos |= tf.equal(next_id, eos_id)\n",
    "\n",
    "      next_id = tf.expand_dims(next_id, axis=1)\n",
    "      decoded_ids = tf.concat([decoded_ids, next_id], axis=1)\n",
    "\n",
    "      return i + 1, hit_eos, next_id, decoded_ids, cache, log_prob\n",
    "\n",
    "    def is_not_finished(i, hit_eos, *_):\n",
    "      finished = i >= decode_length\n",
    "      if not force_decode_length:\n",
    "        finished |= tf.reduce_all(hit_eos)\n",
    "      return tf.logical_not(finished)\n",
    "\n",
    "    decoded_ids = tf.zeros([batch_size, 0], dtype=tf.int64)\n",
    "    hit_eos = tf.fill([batch_size], False)\n",
    "    next_id = sos_id * tf.ones([batch_size, 1], dtype=tf.int64)\n",
    "    initial_log_prob = tf.zeros([batch_size], dtype=tf.float32)\n",
    "    _, _, _, decoded_ids, cache, log_prob = tf.while_loop(\n",
    "        is_not_finished,\n",
    "        inner_loop, [\n",
    "            tf.constant(0), hit_eos, next_id, decoded_ids, cache,\n",
    "            initial_log_prob\n",
    "        ],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape([]),\n",
    "            tf.TensorShape([None]),\n",
    "            tf.TensorShape([None, None]),\n",
    "            tf.TensorShape([None, None]),\n",
    "            nest.map_structure(beam_search.get_state_shape_invariants, cache),\n",
    "            tf.TensorShape([None]),\n",
    "        ])\n",
    "    scores = log_prob\n",
    "\n",
    "  return {\"outputs\": decoded_ids, \"scores\": scores, \"cache\": cache}\n",
    "\n",
    "\n",
    "@registry.register_model\n",
    "class TransformerScorer(Transformer):\n",
    "  \"\"\"Transformer model, but only scores in PREDICT mode.\n",
    "\n",
    "  Checkpoints between Transformer and TransformerScorer are interchangeable.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(TransformerScorer, self).__init__(*args, **kwargs)\n",
    "    self._name = \"transformer\"\n",
    "    self._base_name = \"transformer\"\n",
    "\n",
    "  def infer(self,\n",
    "            features=None,\n",
    "            decode_length=50,\n",
    "            beam_size=1,\n",
    "            top_beams=1,\n",
    "            alpha=0.0,\n",
    "            use_tpu=False):\n",
    "    \"\"\"Returns the targets and their log probabilities.\"\"\"\n",
    "    del decode_length, beam_size, top_beams, alpha, use_tpu\n",
    "    assert features is not None\n",
    "\n",
    "    # Run the model\n",
    "    self.hparams.force_full_predict = True\n",
    "    with tf.variable_scope(self.name):\n",
    "      logits, _ = self.model_fn(features)\n",
    "    assert len(logits.shape) == 5  # [batch, time, 1, 1, vocab]\n",
    "    logits = tf.squeeze(logits, [2, 3])\n",
    "\n",
    "    # Compute the log probabilities\n",
    "    log_probs = common_layers.log_prob_from_logits(logits)\n",
    "\n",
    "    targets = features[\"targets\"]\n",
    "    assert len(targets.shape) == 4  # [batch, time, 1, 1]\n",
    "    targets = tf.squeeze(targets, [2, 3])\n",
    "\n",
    "    # Slice out the log_probs of the targets\n",
    "    log_probs = common_layers.index_last_dim_with_indices(log_probs, targets)\n",
    "\n",
    "    # Sum over time to get the log_prob of the sequence\n",
    "    scores = tf.reduce_sum(log_probs, axis=1)\n",
    "\n",
    "    return {\"outputs\": targets, \"scores\": scores}\n",
    "\n",
    "\n",
    "@registry.register_model\n",
    "class TransformerEncoder(t2t_model.T2TModel):\n",
    "  \"\"\"Transformer, encoder only.\"\"\"\n",
    "\n",
    "  def body(self, features):\n",
    "    hparams = self._hparams\n",
    "    inputs = features[\"inputs\"]\n",
    "    target_space = features[\"target_space_id\"]\n",
    "\n",
    "    inputs = common_layers.flatten4d3d(inputs)\n",
    "\n",
    "    (encoder_input, encoder_self_attention_bias, _) = (\n",
    "        transformer_prepare_encoder(inputs, target_space, hparams))\n",
    "\n",
    "    encoder_input = tf.nn.dropout(encoder_input,\n",
    "                                  1.0 - hparams.layer_prepostprocess_dropout)\n",
    "    encoder_output = transformer_encoder(\n",
    "        encoder_input,\n",
    "        encoder_self_attention_bias,\n",
    "        hparams,\n",
    "        nonpadding=features_to_nonpadding(features, \"inputs\"))\n",
    "    encoder_output = tf.expand_dims(encoder_output, 2)\n",
    "\n",
    "    return encoder_output\n",
    "\n",
    "\n",
    "@registry.register_model\n",
    "class TransformerRegressor(TransformerEncoder):\n",
    "  \"\"\"Transformer inheriting from Encoder, for the regression problem.\n",
    "\n",
    "  Final result is a tensor that has a shape of (?, 1, 1, 1).\n",
    "  \"\"\"\n",
    "\n",
    "  def top(self, body_output, features):\n",
    "    \"\"\"Computes single scalar value from body_output.\"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"reg_top_ffn\"):\n",
    "      x = body_output\n",
    "      x = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
    "      res = tf.layers.dense(x, 1, name=\"model_top\")\n",
    "      return res\n",
    "\n",
    "\n",
    "def features_to_nonpadding(features, inputs_or_targets=\"inputs\"):\n",
    "  key = inputs_or_targets + \"_segmentation\"\n",
    "  if features and key in features:\n",
    "    return tf.minimum(tf.to_float(features[key]), 1.0)\n",
    "  return None\n",
    "\n",
    "\n",
    "def transformer_prepare_decoder(targets, hparams, features=None, pad=None):\n",
    "  \"\"\"Prepare one shard of the model for the decoder.\n",
    "\n",
    "  Args:\n",
    "    targets: a Tensor.\n",
    "    hparams: run hyperparameters\n",
    "    features: optionally pass the entire features dictionary as well. This is\n",
    "      needed now for \"packed\" datasets.\n",
    "    pad: vector to use for padding when shifting targets right\n",
    "\n",
    "  Returns:\n",
    "    decoder_input: a Tensor, bottom of decoder stack\n",
    "    decoder_self_attention_bias: a bias tensor for use in decoder self-attention\n",
    "  \"\"\"\n",
    "  if hparams.causal_decoder_self_attention:\n",
    "    # Causal attention.\n",
    "    if hparams.prepend_mode == \"prepend_inputs_full_attention\":\n",
    "      decoder_self_attention_bias = (\n",
    "          common_attention.attention_bias_prepend_inputs_full_attention(\n",
    "              common_attention.embedding_to_padding(targets)))\n",
    "    else:\n",
    "      decoder_self_attention_bias = (\n",
    "          common_attention.attention_bias_lower_triangle(\n",
    "              common_layers.shape_list(targets)[1]))\n",
    "  else:\n",
    "    # Full attention.\n",
    "    decoder_padding = common_attention.embedding_to_padding(targets)\n",
    "    decoder_self_attention_bias = (\n",
    "        common_attention.attention_bias_ignore_padding(decoder_padding))\n",
    "\n",
    "  if features and \"targets_segmentation\" in features:\n",
    "    # \"Packed\" dataset - keep the examples from seeing each other.\n",
    "    targets_segmentation = features[\"targets_segmentation\"]\n",
    "    targets_position = features[\"targets_position\"]\n",
    "    decoder_self_attention_bias += common_attention.attention_bias_same_segment(\n",
    "        targets_segmentation, targets_segmentation)\n",
    "  else:\n",
    "    targets_position = None\n",
    "  if hparams.proximity_bias:\n",
    "    decoder_self_attention_bias += common_attention.attention_bias_proximal(\n",
    "        common_layers.shape_list(targets)[1])\n",
    "  decoder_input = common_layers.shift_right_3d(targets, pad)\n",
    "  if hparams.pos == \"timing\":\n",
    "    if targets_position is not None:\n",
    "      decoder_input = common_attention.add_timing_signal_1d_given_position(\n",
    "          decoder_input, targets_position)\n",
    "    else:\n",
    "      decoder_input = common_attention.add_timing_signal_1d(decoder_input)\n",
    "  elif hparams.pos == \"timing_from_features\":\n",
    "    decoder_input = common_attention.add_timing_signals_from_features(\n",
    "        decoder_input, features, hparams.position_features)\n",
    "  elif hparams.pos == \"emb\":\n",
    "    decoder_input = common_attention.add_positional_embedding(\n",
    "        decoder_input, hparams.max_length, \"targets_positional_embedding\",\n",
    "        targets_position)\n",
    "\n",
    "  if hparams.activation_dtype == \"bfloat16\":\n",
    "    decoder_self_attention_bias = tf.cast(decoder_self_attention_bias,\n",
    "                                          tf.bfloat16)\n",
    "  return (decoder_input, decoder_self_attention_bias)\n",
    "\n",
    "\n",
    "def transformer_self_attention_layer(decoder_input,\n",
    "                                     decoder_self_attention_bias,\n",
    "                                     layer_idx,\n",
    "                                     hparams,\n",
    "                                     encoder_output=None,\n",
    "                                     encoder_decoder_attention_bias=None,\n",
    "                                     cache=None,\n",
    "                                     decode_loop_step=None,\n",
    "                                     save_weights_to=None,\n",
    "                                     make_image_summary=False,\n",
    "                                     layer_collection=None,\n",
    "                                     recurrent_memory_by_layer=None,\n",
    "                                     chunk_number=None):\n",
    "  \"\"\"A single transformer self-attention layer.\"\"\"\n",
    "  x = decoder_input\n",
    "  layer = layer_idx\n",
    "  layer_name = \"layer_%d\" % layer\n",
    "  layer_cache = cache[layer_name] if cache is not None else None\n",
    "\n",
    "  attention_dropout_broadcast_dims = (\n",
    "      common_layers.comma_separated_string_to_integer_list(\n",
    "          getattr(hparams, \"attention_dropout_broadcast_dims\", \"\")))\n",
    "\n",
    "  if recurrent_memory_by_layer is not None:\n",
    "    recurrent_memory = recurrent_memory_by_layer[layer_name]\n",
    "  else:\n",
    "    recurrent_memory = None\n",
    "\n",
    "  if layer < hparams.get(\"num_area_layers\", 0):\n",
    "    max_area_width = hparams.get(\"max_area_width\", 1)\n",
    "    max_area_height = hparams.get(\"max_area_height\", 1)\n",
    "    memory_height = hparams.get(\"max_area_height\", 1)\n",
    "  else:\n",
    "    max_area_width = 1\n",
    "    max_area_height = 1\n",
    "    memory_height = 1\n",
    "  with tf.variable_scope(layer_name):\n",
    "    with tf.variable_scope(\"self_attention\"):\n",
    "      y = common_attention.multihead_attention(\n",
    "          common_layers.layer_preprocess(\n",
    "              x, hparams, layer_collection=layer_collection),\n",
    "          None,\n",
    "          decoder_self_attention_bias,\n",
    "          hparams.attention_key_channels or hparams.hidden_size,\n",
    "          hparams.attention_value_channels or hparams.hidden_size,\n",
    "          hparams.hidden_size,\n",
    "          hparams.num_heads,\n",
    "          hparams.attention_dropout,\n",
    "          attention_type=hparams.self_attention_type,\n",
    "          max_relative_position=hparams.max_relative_position,\n",
    "          heads_share_relative_embedding=(\n",
    "              hparams.heads_share_relative_embedding),\n",
    "          add_relative_to_values=hparams.add_relative_to_values,\n",
    "          save_weights_to=save_weights_to,\n",
    "          cache=layer_cache,\n",
    "          make_image_summary=make_image_summary,\n",
    "          dropout_broadcast_dims=attention_dropout_broadcast_dims,\n",
    "          max_length=hparams.get(\"max_length\"),\n",
    "          decode_loop_step=decode_loop_step,\n",
    "          vars_3d=hparams.get(\"attention_variables_3d\"),\n",
    "          activation_dtype=hparams.get(\"activation_dtype\", \"float32\"),\n",
    "          weight_dtype=hparams.get(\"weight_dtype\", \"float32\"),\n",
    "          layer_collection=layer_collection,\n",
    "          recurrent_memory=recurrent_memory,\n",
    "          chunk_number=chunk_number,\n",
    "          hard_attention_k=hparams.get(\"hard_attention_k\", 0),\n",
    "          gumbel_noise_weight=hparams.get(\"gumbel_noise_weight\", 0.0),\n",
    "          max_area_width=max_area_width,\n",
    "          max_area_height=max_area_height,\n",
    "          memory_height=memory_height,\n",
    "          area_key_mode=hparams.get(\"area_key_mode\", \"none\"),\n",
    "          area_value_mode=hparams.get(\"area_value_mode\", \"none\"),\n",
    "          training=(hparams.get(\n",
    "              \"mode\",\n",
    "              tf_estimator.ModeKeys.TRAIN) == tf_estimator.ModeKeys.TRAIN))\n",
    "      x = common_layers.layer_postprocess(x, y, hparams)\n",
    "    if encoder_output is not None:\n",
    "      if not isinstance(encoder_output, (list,)):\n",
    "        encoder_output = [encoder_output]\n",
    "      with tf.variable_scope(\"encdec_attention\"):\n",
    "        for enc_output in encoder_output:\n",
    "          y = common_attention.multihead_attention(\n",
    "              common_layers.layer_preprocess(\n",
    "                  x, hparams, layer_collection=layer_collection),\n",
    "              enc_output,\n",
    "              encoder_decoder_attention_bias,\n",
    "              hparams.attention_key_channels or hparams.hidden_size,\n",
    "              hparams.attention_value_channels or hparams.hidden_size,\n",
    "              hparams.hidden_size,\n",
    "              hparams.num_heads,\n",
    "              hparams.attention_dropout,\n",
    "              max_relative_position=hparams.max_relative_position,\n",
    "              heads_share_relative_embedding=(\n",
    "                  hparams.heads_share_relative_embedding),\n",
    "              add_relative_to_values=hparams.add_relative_to_values,\n",
    "              save_weights_to=save_weights_to,\n",
    "              cache=layer_cache,\n",
    "              make_image_summary=make_image_summary,\n",
    "              dropout_broadcast_dims=attention_dropout_broadcast_dims,\n",
    "              max_length=hparams.get(\"max_length\"),\n",
    "              vars_3d=hparams.get(\"attention_variables_3d\"),\n",
    "              activation_dtype=hparams.get(\"activation_dtype\", \"float32\"),\n",
    "              weight_dtype=hparams.get(\"weight_dtype\", \"float32\"),\n",
    "              layer_collection=layer_collection,\n",
    "              hard_attention_k=hparams.get(\"hard_attention_k\", 0),\n",
    "              gumbel_noise_weight=hparams.get(\"gumbel_noise_weight\", 0.0),\n",
    "              max_area_width=max_area_width,\n",
    "              max_area_height=max_area_height,\n",
    "              memory_height=memory_height,\n",
    "              area_key_mode=hparams.get(\"area_key_mode\", \"none\"),\n",
    "              area_value_mode=hparams.get(\"area_value_mode\", \"none\"),\n",
    "              training=(hparams.get(\n",
    "                  \"mode\",\n",
    "                  tf_estimator.ModeKeys.TRAIN) == tf_estimator.ModeKeys.TRAIN))\n",
    "          x = common_layers.layer_postprocess(x, y, hparams)\n",
    "    return x, layer_cache\n",
    "\n",
    "\n",
    "def transformer_decoder_layer(decoder_input,\n",
    "                              decoder_self_attention_bias,\n",
    "                              layer_idx,\n",
    "                              hparams,\n",
    "                              encoder_output=None,\n",
    "                              encoder_decoder_attention_bias=None,\n",
    "                              cache=None,\n",
    "                              decode_loop_step=None,\n",
    "                              nonpadding=None,\n",
    "                              save_weights_to=None,\n",
    "                              make_image_summary=False,\n",
    "                              losses=None,\n",
    "                              layer_collection=None,\n",
    "                              recurrent_memory_by_layer=None,\n",
    "                              chunk_number=None):\n",
    "  \"\"\"A single transformer decoder layer.\"\"\"\n",
    "  x, layer_cache = transformer_self_attention_layer(\n",
    "      decoder_input=decoder_input,\n",
    "      decoder_self_attention_bias=decoder_self_attention_bias,\n",
    "      layer_idx=layer_idx,\n",
    "      hparams=hparams,\n",
    "      encoder_output=encoder_output,\n",
    "      encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n",
    "      cache=cache,\n",
    "      decode_loop_step=decode_loop_step,\n",
    "      save_weights_to=save_weights_to,\n",
    "      make_image_summary=make_image_summary,\n",
    "      layer_collection=layer_collection,\n",
    "      recurrent_memory_by_layer=recurrent_memory_by_layer,\n",
    "      chunk_number=chunk_number)\n",
    "\n",
    "  layer = layer_idx\n",
    "  layer_name = \"layer_%d\" % layer\n",
    "  with tf.variable_scope(layer_name):\n",
    "    with tf.variable_scope(\"ffn\"):\n",
    "      y = transformer_ffn_layer(\n",
    "          common_layers.layer_preprocess(\n",
    "              x, hparams, layer_collection=layer_collection),\n",
    "          hparams,\n",
    "          conv_padding=\"LEFT\",\n",
    "          nonpadding_mask=nonpadding,\n",
    "          losses=losses,\n",
    "          cache=layer_cache,\n",
    "          decode_loop_step=decode_loop_step,\n",
    "          layer_collection=layer_collection)\n",
    "      x = common_layers.layer_postprocess(x, y, hparams)\n",
    "      return x\n",
    "\n",
    "\n",
    "def transformer_decoder(decoder_input,\n",
    "                        encoder_output,\n",
    "                        decoder_self_attention_bias,\n",
    "                        encoder_decoder_attention_bias,\n",
    "                        hparams,\n",
    "                        cache=None,\n",
    "                        decode_loop_step=None,\n",
    "                        name=\"decoder\",\n",
    "                        nonpadding=None,\n",
    "                        save_weights_to=None,\n",
    "                        make_image_summary=True,\n",
    "                        losses=None,\n",
    "                        layer_collection=None,\n",
    "                        recurrent_memory_by_layer=None,\n",
    "                        chunk_number=None):\n",
    "  \"\"\"A stack of transformer layers.\n",
    "\n",
    "  Args:\n",
    "    decoder_input: a Tensor\n",
    "    encoder_output: a Tensor\n",
    "    decoder_self_attention_bias: bias Tensor for self-attention (see\n",
    "      common_attention.attention_bias())\n",
    "    encoder_decoder_attention_bias: bias Tensor for encoder-decoder attention\n",
    "      (see common_attention.attention_bias())\n",
    "    hparams: hyperparameters for model\n",
    "    cache: dict, containing tensors which are the results of previous\n",
    "      attentions, used for fast decoding.\n",
    "    decode_loop_step: An integer, step number of the decoding loop. Only used\n",
    "      for inference on TPU.\n",
    "    name: a string\n",
    "    nonpadding: optional Tensor with shape [batch_size, encoder_length]\n",
    "      indicating what positions are not padding.  This is used to mask out\n",
    "      padding in convolutional layers.  We generally only need this mask for\n",
    "      \"packed\" datasets, because for ordinary datasets, no padding is ever\n",
    "      followed by nonpadding.\n",
    "    save_weights_to: an optional dictionary to capture attention weights for\n",
    "      visualization; the weights tensor will be appended there under a string\n",
    "      key created from the variable scope (including name).\n",
    "    make_image_summary: Whether to make an attention image summary.\n",
    "    losses: optional list onto which to append extra training losses\n",
    "    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the KFAC\n",
    "      optimizer. Default is None.\n",
    "    recurrent_memory_by_layer: Optional dict, mapping layer names to instances\n",
    "      of transformer_memory.RecurrentMemory. Default is None.\n",
    "    chunk_number: an optional integer Tensor with shape [batch] used to operate\n",
    "      the recurrent_memory.\n",
    "\n",
    "  Returns:\n",
    "    y: a Tensors\n",
    "  \"\"\"\n",
    "  x = decoder_input\n",
    "\n",
    "  mlperf_log.transformer_print(\n",
    "      key=mlperf_log.MODEL_HP_NUM_HIDDEN_LAYERS,\n",
    "      value=hparams.num_decoder_layers or hparams.num_hidden_layers,\n",
    "      hparams=hparams)\n",
    "  mlperf_log.transformer_print(\n",
    "      key=mlperf_log.MODEL_HP_ATTENTION_DROPOUT,\n",
    "      value=hparams.attention_dropout,\n",
    "      hparams=hparams)\n",
    "  mlperf_log.transformer_print(\n",
    "      key=mlperf_log.MODEL_HP_ATTENTION_DENSE,\n",
    "      value={\n",
    "          \"use_bias\": \"false\",\n",
    "          \"num_heads\": hparams.num_heads,\n",
    "          \"hidden_size\": hparams.hidden_size\n",
    "      },\n",
    "      hparams=hparams)\n",
    "\n",
    "  with tf.variable_scope(name):\n",
    "    for layer_idx in range(hparams.num_decoder_layers or\n",
    "                           hparams.num_hidden_layers):\n",
    "      x = transformer_decoder_layer(\n",
    "          x,\n",
    "          decoder_self_attention_bias,\n",
    "          layer_idx,\n",
    "          hparams,\n",
    "          encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n",
    "          encoder_output=encoder_output,\n",
    "          cache=cache,\n",
    "          decode_loop_step=decode_loop_step,\n",
    "          nonpadding=nonpadding,\n",
    "          save_weights_to=save_weights_to,\n",
    "          make_image_summary=make_image_summary,\n",
    "          losses=losses,\n",
    "          layer_collection=layer_collection,\n",
    "          recurrent_memory_by_layer=recurrent_memory_by_layer,\n",
    "          chunk_number=chunk_number\n",
    "          )\n",
    "\n",
    "    # if normalization is done in layer_preprocess, then it should also be done\n",
    "    # on the output, since the output can grow very large, being the sum of\n",
    "    # a whole stack of unnormalized layer outputs.\n",
    "    mlperf_log.transformer_print(\n",
    "        key=mlperf_log.MODEL_HP_NORM,\n",
    "        value={\"hidden_size\": hparams.hidden_size})\n",
    "    return common_layers.layer_preprocess(\n",
    "        x, hparams, layer_collection=layer_collection)\n",
    "\n",
    "\n",
    "@registry.register_model\n",
    "class TransformerMemory(Transformer):\n",
    "  \"\"\"Transformer language model with memory across chunks.\"\"\"\n",
    "\n",
    "  # TODO(kitaev): consider overriding set_mode to swap out recurrent memory when\n",
    "  # switching between training and evaluation.\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(TransformerMemory, self).__init__(*args, **kwargs)\n",
    "\n",
    "    hparams = self._hparams\n",
    "    self.recurrent_memory_by_layer = {}\n",
    "    for layer in range(hparams.num_decoder_layers or hparams.num_hidden_layers):\n",
    "      layer_name = \"layer_%d\" % layer\n",
    "      if hparams.memory_type == \"neural_memory\":\n",
    "        memory = transformer_memory.TransformerMemory(\n",
    "            batch_size=int(hparams.batch_size / hparams.max_length),\n",
    "            key_depth=hparams.hidden_size,\n",
    "            val_depth=hparams.hidden_size,\n",
    "            memory_size=hparams.split_targets_chunk_length,\n",
    "            sharpen_factor=1.,\n",
    "            name=layer_name + \"/recurrent_memory\")\n",
    "      elif hparams.memory_type == \"transformer_xl\":\n",
    "        memory = transformer_memory.RecentTokensMemory(\n",
    "            layer_name + \"/recurrent_memory\", hparams)\n",
    "      else:\n",
    "        raise ValueError(\"Unsupported memory type: %s\" % hparams.memory_type)\n",
    "      self.recurrent_memory_by_layer[layer_name] = memory\n",
    "\n",
    "  @property\n",
    "  def has_input(self):\n",
    "    if hasattr(self._hparams, \"unconditional\") and self._hparams.unconditional:\n",
    "      return False\n",
    "    return super(TransformerMemory, self).has_input\n",
    "\n",
    "  def _beam_decode(self, features, decode_length, beam_size, top_beams, alpha,\n",
    "                   use_tpu=False):\n",
    "    \"\"\"Overriding beam search because for now only the slow version works with\n",
    "    memory\n",
    "    \"\"\"\n",
    "    return self._beam_decode_slow(features, decode_length, beam_size,\n",
    "                                  top_beams, alpha, use_tpu)\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base_v1():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = common_hparams.basic_params1()\n",
    "  hparams.norm_type = \"layer\"\n",
    "  hparams.hidden_size = 512\n",
    "  hparams.batch_size = 4096\n",
    "  hparams.max_length = 256\n",
    "  hparams.clip_grad_norm = 0.  # i.e. no gradient clipping\n",
    "  hparams.optimizer_adam_epsilon = 1e-9\n",
    "  hparams.learning_rate_schedule = \"legacy\"\n",
    "  hparams.learning_rate_decay_scheme = \"noam\"\n",
    "  hparams.learning_rate = 0.1\n",
    "  hparams.learning_rate_warmup_steps = 4000\n",
    "  hparams.initializer_gain = 1.0\n",
    "  hparams.num_hidden_layers = 6\n",
    "  hparams.initializer = \"uniform_unit_scaling\"\n",
    "  hparams.weight_decay = 0.0\n",
    "  hparams.optimizer_adam_beta1 = 0.9\n",
    "  hparams.optimizer_adam_beta2 = 0.98\n",
    "  hparams.num_sampled_classes = 0\n",
    "  hparams.label_smoothing = 0.1\n",
    "  hparams.shared_embedding_and_softmax_weights = True\n",
    "  hparams.symbol_modality_num_shards = 16\n",
    "\n",
    "  # Add new ones like this.\n",
    "  hparams.add_hparam(\"filter_size\", 2048)\n",
    "  # Layer-related flags. If zero, these fall back on hparams.num_hidden_layers.\n",
    "  hparams.add_hparam(\"num_encoder_layers\", 0)\n",
    "  hparams.add_hparam(\"num_decoder_layers\", 0)\n",
    "  # Attention-related flags.\n",
    "  hparams.add_hparam(\"num_heads\", 8)\n",
    "  hparams.add_hparam(\"attention_key_channels\", 0)\n",
    "  hparams.add_hparam(\"attention_value_channels\", 0)\n",
    "  hparams.add_hparam(\"ffn_layer\", \"dense_relu_dense\")\n",
    "  hparams.add_hparam(\"parameter_attention_key_channels\", 0)\n",
    "  hparams.add_hparam(\"parameter_attention_value_channels\", 0)\n",
    "  # All hyperparameters ending in \"dropout\" are automatically set to 0.0\n",
    "  # when not in training mode.\n",
    "  hparams.add_hparam(\"attention_dropout\", 0.0)\n",
    "  hparams.add_hparam(\"attention_dropout_broadcast_dims\", \"\")\n",
    "  hparams.add_hparam(\"relu_dropout\", 0.0)\n",
    "  hparams.add_hparam(\"relu_dropout_broadcast_dims\", \"\")\n",
    "  hparams.add_hparam(\"pos\", \"timing\")  # timing, none\n",
    "  hparams.add_hparam(\"position_features\", \"\")\n",
    "  hparams.add_hparam(\"nbr_decoder_problems\", 1)\n",
    "  hparams.add_hparam(\"proximity_bias\", False)\n",
    "  hparams.add_hparam(\"causal_decoder_self_attention\", True)\n",
    "  hparams.add_hparam(\"use_pad_remover\", True)\n",
    "  hparams.add_hparam(\"self_attention_type\", \"dot_product\")\n",
    "  hparams.add_hparam(\"conv_first_kernel\", 3)\n",
    "  hparams.add_hparam(\"attention_variables_3d\", False)\n",
    "  hparams.add_hparam(\"use_target_space_embedding\", True)\n",
    "  # These parameters are only used when ffn_layer==\"local_moe_tpu\"\n",
    "  hparams.add_hparam(\"moe_overhead_train\", 1.0)\n",
    "  hparams.add_hparam(\"moe_overhead_eval\", 2.0)\n",
    "  hparams.moe_num_experts = 16\n",
    "  hparams.moe_loss_coef = 1e-3\n",
    "  # If specified, use this value instead of problem name in metrics.py.\n",
    "  # This is useful for programs that can automatically compare experiments side\n",
    "  #   by side based on the same metric names.\n",
    "  hparams.add_hparam(\"overload_eval_metric_name\", \"\")\n",
    "  # For making a transformer encoder unidirectional by using masked\n",
    "  # attention.\n",
    "  hparams.add_hparam(\"unidirectional_encoder\", False)\n",
    "  # For hard attention.\n",
    "  hparams.add_hparam(\"hard_attention_k\", 0)\n",
    "  hparams.add_hparam(\"gumbel_noise_weight\", 0.0)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base_v2():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_base_v1()\n",
    "  hparams.layer_preprocess_sequence = \"n\"\n",
    "  hparams.layer_postprocess_sequence = \"da\"\n",
    "  hparams.layer_prepostprocess_dropout = 0.1\n",
    "  hparams.attention_dropout = 0.1\n",
    "  hparams.relu_dropout = 0.1\n",
    "  hparams.learning_rate_warmup_steps = 8000\n",
    "  hparams.learning_rate = 0.2\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base_vq_ada_32ex_packed():\n",
    "  \"\"\"Set of hyperparameters for lm1b packed following tpu params.\"\"\"\n",
    "  hparams = transformer_base_v2()\n",
    "  expert_utils.update_hparams_for_vq_gating(hparams)\n",
    "  hparams.moe_num_experts = 32\n",
    "  hparams.gating_type = \"vq\"\n",
    "  # this gives us a batch size of 16 because each seq is len 256\n",
    "  hparams.batch_size = 5072\n",
    "  hparams.ffn_layer = \"local_moe\"\n",
    "  hparams.shared_embedding_and_softmax_weights = False\n",
    "  hparams.learning_rate_warmup_steps = 10000\n",
    "  # one epoch for languagemodel_lm1b32k_packed = 27200 steps w/ bsize 128\n",
    "  hparams.learning_rate_decay_steps = 27200\n",
    "  hparams.num_heads = 4\n",
    "  hparams.num_blocks = 1\n",
    "  hparams.moe_k = 1\n",
    "  hparams.num_decoder_layers = 6\n",
    "  hparams.label_smoothing = 0.\n",
    "  hparams.layer_prepostprocess_dropout = 0.1\n",
    "  hparams.layer_postprocess_sequence = \"dan\"\n",
    "  hparams.layer_preprocess_sequence = \"none\"\n",
    "  hparams.weight_decay = 1e-06\n",
    "  hparams.attention_dropout = 0.1\n",
    "  hparams.optimizer = \"Adafactor\"\n",
    "  hparams.learning_rate_schedule = \"linear_warmup*rsqrt_decay*linear_decay\"\n",
    "  hparams.activation_dtype = \"float32\"\n",
    "  hparams.learning_rate = 0.1\n",
    "  hparams.learning_rate_constant = 1.0\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_topk_16_packed():\n",
    "  hparams = transformer_base_vq_ada_32ex_packed()\n",
    "  hparams.gating_type = \"topk\"\n",
    "  hparams.moe_num_experts = 16\n",
    "  hparams.moe_k = 2\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base_vq1_16_nb1_packed_nda_b01_scales():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_base_vq_ada_32ex_packed()\n",
    "  hparams.use_scales = int(True)\n",
    "  hparams.moe_num_experts = 16\n",
    "  hparams.moe_k = 1\n",
    "  hparams.beta = 0.1\n",
    "  hparams.layer_preprocess_sequence = \"n\"\n",
    "  hparams.layer_postprocess_sequence = \"da\"\n",
    "  hparams.ema = False\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base_vq1_16_nb1_packed_dan_b01_scales():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_base_vq_ada_32ex_packed()\n",
    "  hparams.use_scales = int(True)\n",
    "  hparams.moe_num_experts = 16\n",
    "  hparams.moe_k = 1\n",
    "  hparams.beta = 0.1\n",
    "  hparams.ema = False\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base_vq1_16_nb1_packed_nda_b01_scales_dialog():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_base_vq1_16_nb1_packed_nda_b01_scales()\n",
    "  hparams.batch_size = 2048\n",
    "  hparams.max_length = 1024\n",
    "  hparams.filter_size = 3072\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ada_lmpackedbase():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_base_vq_ada_32ex_packed()\n",
    "  hparams.ffn_layer = \"dense_relu_dense\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ada_lmpackedbase_dialog():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_base_vq_ada_32ex_packed()\n",
    "  hparams.max_length = 1024\n",
    "  hparams.ffn_layer = \"dense_relu_dense\"\n",
    "  hparams.batch_size = 4096\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ada_lmpackedbase_relative():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_base_vq_ada_32ex_packed()\n",
    "  hparams.ffn_layer = \"dense_relu_dense\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base_v3():\n",
    "  \"\"\"Base parameters for Transformer model.\"\"\"\n",
    "  # Update parameters here, then occasionally cut a versioned set, e.g.\n",
    "  # transformer_base_v2.\n",
    "  hparams = transformer_base_v2()\n",
    "  hparams.optimizer_adam_beta2 = 0.997\n",
    "  # New way of specifying learning rate schedule.\n",
    "  # Equivalent to previous version.\n",
    "  hparams.learning_rate_schedule = (\n",
    "      \"constant*linear_warmup*rsqrt_decay*rsqrt_hidden_size\")\n",
    "  hparams.learning_rate_constant = 2.0\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base():\n",
    "  \"\"\"Base parameters for Transformer model.\"\"\"\n",
    "  hparams = transformer_base_v3()\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_big():\n",
    "  \"\"\"HParams for transformer big model on WMT.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "  hparams.hidden_size = 1024\n",
    "  hparams.filter_size = 4096\n",
    "  # Reduce batch size to 2048 from 4096 to be able to train the model on a GPU\n",
    "  # with 12 GB memory. For example, NVIDIA TITAN V GPU.\n",
    "  hparams.batch_size = 2048\n",
    "  hparams.num_heads = 16\n",
    "  hparams.layer_prepostprocess_dropout = 0.3\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall():\n",
    "  \"\"\"Hparams for transformer on LM for pretraining/finetuning/mixing.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "  hparams.batch_size = 2048\n",
    "  hparams.hidden_size = 768\n",
    "  hparams.filter_size = 3072\n",
    "  hparams.num_hidden_layers = 12\n",
    "  hparams.num_heads = 12\n",
    "  hparams.label_smoothing = 0.0\n",
    "  hparams.max_length = 1024\n",
    "  hparams.eval_drop_long_sequences = True\n",
    "  hparams.multiproblem_mixing_schedule = \"pretrain\"\n",
    "  hparams.multiproblem_vocab_size = 65536\n",
    "  hparams.clip_grad_norm = 1.0\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_finetune_tied():\n",
    "  \"\"\"Tied means fine-tune CNN/DM summarization as LM.\"\"\"\n",
    "  hparams = transformer_tall()\n",
    "  hparams.multiproblem_max_input_length = 750\n",
    "  hparams.multiproblem_max_target_length = 100\n",
    "  hparams.multiproblem_schedule_max_examples = 0\n",
    "  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n",
    "  hparams.learning_rate_constant = 5e-5\n",
    "  hparams.learning_rate_warmup_steps = 100\n",
    "  # Set train steps to learning_rate_decay_steps or less\n",
    "  hparams.learning_rate_decay_steps = 80000\n",
    "  hparams.multiproblem_target_eval_only = True\n",
    "  hparams.multiproblem_reweight_label_loss = True\n",
    "  hparams.multiproblem_label_weight = 1.0\n",
    "  hparams.optimizer = \"true_adam\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_train_tied():\n",
    "  \"\"\"Tied means train CNN/DM summarization as LM.\"\"\"\n",
    "  hparams = transformer_tall()\n",
    "  hparams.multiproblem_max_input_length = 750\n",
    "  hparams.multiproblem_max_target_length = 100\n",
    "  hparams.multiproblem_schedule_max_examples = 0\n",
    "  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n",
    "  hparams.learning_rate_constant = 2e-4\n",
    "  hparams.learning_rate_warmup_steps = 8000\n",
    "  # Set train steps to learning_rate_decay_steps or less\n",
    "  hparams.learning_rate_decay_steps = 150000\n",
    "  hparams.multiproblem_target_eval_only = True\n",
    "  hparams.multiproblem_reweight_label_loss = True\n",
    "  hparams.multiproblem_label_weight = 1.0\n",
    "  hparams.optimizer = \"true_adam\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_finetune_uniencdec():\n",
    "  \"\"\"Fine-tune CNN/DM with a unidirectional encoder and decoder.\"\"\"\n",
    "  hparams = transformer_tall()\n",
    "  hparams.max_input_seq_length = 750\n",
    "  hparams.max_target_seq_length = 100\n",
    "  hparams.optimizer = \"true_adam\"\n",
    "  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n",
    "  hparams.learning_rate_decay_steps = 80000\n",
    "  hparams.learning_rate_constant = 5e-5\n",
    "  hparams.learning_rate_warmup_steps = 100\n",
    "  hparams.unidirectional_encoder = True\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_train_uniencdec():\n",
    "  \"\"\"Train CNN/DM with a unidirectional encoder and decoder.\"\"\"\n",
    "  hparams = transformer_tall()\n",
    "  hparams.max_input_seq_length = 750\n",
    "  hparams.max_target_seq_length = 100\n",
    "  hparams.optimizer = \"true_adam\"\n",
    "  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n",
    "  hparams.learning_rate_decay_steps = 150000\n",
    "  hparams.learning_rate_constant = 2e-4\n",
    "  hparams.unidirectional_encoder = True\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_finetune_textclass():\n",
    "  \"\"\"Hparams for transformer on LM for finetuning on text class problems.\"\"\"\n",
    "  hparams = transformer_tall()\n",
    "  hparams.learning_rate_constant = 6.25e-5\n",
    "  hparams.learning_rate_schedule = (\"linear_warmup*constant*linear_decay\")\n",
    "  hparams.multiproblem_schedule_max_examples = 0\n",
    "  hparams.multiproblem_target_eval_only = True\n",
    "  hparams.learning_rate_warmup_steps = 50\n",
    "  # Set train steps to learning_rate_decay_steps or less\n",
    "  hparams.learning_rate_decay_steps = 25000\n",
    "  hparams.multiproblem_reweight_label_loss = True\n",
    "  hparams.multiproblem_label_weight = 0.95\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_pretrain_lm():\n",
    "  \"\"\"Hparams for transformer on LM pretraining (with 64k vocab).\"\"\"\n",
    "  hparams = transformer_tall()\n",
    "  hparams.learning_rate_constant = 2e-4\n",
    "  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n",
    "  hparams.optimizer = \"adam_w\"\n",
    "  hparams.weight_decay = 0.01 * hparams.learning_rate_constant\n",
    "  hparams.optimizer_adam_beta1 = 0.9\n",
    "  hparams.optimizer_adam_beta2 = 0.999\n",
    "  hparams.optimizer_adam_epsilon = 1e-8\n",
    "  # Set max examples to something big when pretraining only the LM, definitely\n",
    "  # something an order of magnitude bigger than number of train steps.\n",
    "  hparams.multiproblem_schedule_max_examples = 5e8\n",
    "  # Set train steps to learning_rate_decay_steps or less\n",
    "  hparams.learning_rate_decay_steps = 5000000\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_pretrain_lm_tpu_adafactor():\n",
    "  \"\"\"Hparams for transformer on LM pretraining (with 64k vocab) on TPU.\"\"\"\n",
    "  hparams = transformer_tall_pretrain_lm()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "  hparams.max_length = 1024\n",
    "  # For multi-problem on TPU we need it in absolute examples.\n",
    "  hparams.batch_size = 8\n",
    "  hparams.multiproblem_vocab_size = 2**16\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_pretrain_lm_tpu_adafactor_large():\n",
    "  \"\"\"Hparams for transformer on LM pretraining on TPU, large model.\"\"\"\n",
    "  hparams = transformer_tall_pretrain_lm_tpu_adafactor()\n",
    "  hparams.hidden_size = 1024\n",
    "  hparams.num_heads = 16\n",
    "  hparams.filter_size = 32768  # max fitting in 16G memory is 49152, batch 2\n",
    "  hparams.batch_size = 4\n",
    "  hparams.multiproblem_mixing_schedule = \"constant\"\n",
    "  # Task order: lm/en-de/en-fr/en-ro/de-en/fr-en/ro-en/cnndm/mnli/squad.\n",
    "  hparams.multiproblem_per_task_threshold = \"320,80,160,1,80,160,2,20,10,5\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_pretrain_lm_tpu():\n",
    "  \"\"\"Hparams for transformer on LM pretraining on TPU with AdamW.\"\"\"\n",
    "  hparams = transformer_tall_pretrain_lm_tpu_adafactor()\n",
    "  # Optimizer gets reset in update_hparams_for_tpu so we set it again here.\n",
    "  hparams.learning_rate_constant = 2e-4\n",
    "  hparams.learning_rate_schedule = (\"linear_warmup * constant * cosdecay\")\n",
    "  hparams.optimizer = \"adam_w\"\n",
    "  hparams.weight_decay = 0.01 * hparams.learning_rate_constant\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tall_big():\n",
    "  \"\"\"Hparams for transformer on LM+MNLI.\"\"\"\n",
    "  hparams = transformer_tall()\n",
    "  hparams.num_hidden_layers = 18\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_big_single_gpu():\n",
    "  \"\"\"HParams for transformer big model for single GPU.\"\"\"\n",
    "  hparams = transformer_big()\n",
    "  hparams.layer_prepostprocess_dropout = 0.1\n",
    "  hparams.learning_rate_warmup_steps = 16000\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base_single_gpu():\n",
    "  \"\"\"HParams for transformer base model for single GPU.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "  hparams.batch_size = 1024\n",
    "  hparams.learning_rate_schedule = \"constant*linear_warmup*rsqrt_decay\"\n",
    "  hparams.learning_rate_constant = 0.1\n",
    "  hparams.learning_rate_warmup_steps = 16000\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_base_multistep8():\n",
    "  \"\"\"HParams for simulating 8 GPUs with MultistepAdam optimizer.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "  hparams.optimizer = \"multistep_adam\"\n",
    "  hparams.optimizer_multistep_accumulate_steps = 8\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_cubbitt():\n",
    "  \"\"\"Transformer hyperparameters used in CUBBITT experiments.\"\"\"\n",
    "  hparams = transformer_big_single_gpu()\n",
    "  hparams.learning_rate_schedule = \"rsqrt_decay\"\n",
    "  hparams.batch_size = 2900\n",
    "  hparams.learning_rate_warmup_steps = 8000\n",
    "  hparams.max_length = 150\n",
    "  hparams.layer_prepostprocess_dropout = 0\n",
    "  hparams.optimizer = \"Adafactor\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_parsing_base():\n",
    "  \"\"\"HParams for parsing on WSJ only.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "  hparams.attention_dropout = 0.2\n",
    "  hparams.layer_prepostprocess_dropout = 0.2\n",
    "  hparams.max_length = 512\n",
    "  hparams.learning_rate_warmup_steps = 16000\n",
    "  hparams.hidden_size = 1024\n",
    "  hparams.learning_rate = 0.05\n",
    "  hparams.shared_embedding_and_softmax_weights = False\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_parsing_big():\n",
    "  \"\"\"HParams for parsing on WSJ semi-supervised.\"\"\"\n",
    "  hparams = transformer_big()\n",
    "  hparams.max_length = 512\n",
    "  hparams.shared_source_target_embedding = False\n",
    "  hparams.learning_rate_warmup_steps = 4000\n",
    "  hparams.layer_prepostprocess_dropout = 0.1\n",
    "  hparams.batch_size = 2048\n",
    "  hparams.learning_rate = 0.05\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_parsing_ice():\n",
    "  \"\"\"HParams for parsing and tagging Icelandic text.\"\"\"\n",
    "  hparams = transformer_base_single_gpu()\n",
    "  hparams.batch_size = 4096\n",
    "  hparams.shared_embedding_and_softmax_weights = False\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tiny():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_test():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 16\n",
    "  hparams.filter_size = 8\n",
    "  hparams.num_heads = 2\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_small():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 256\n",
    "  hparams.filter_size = 1024\n",
    "  hparams.num_heads = 4\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_l2():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_l4():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_hidden_layers = 4\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_l8():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_hidden_layers = 8\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_l10():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_hidden_layers = 10\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_h1():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_heads = 1\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_h4():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_heads = 4\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_h16():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_heads = 16\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_h32():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_heads = 32\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_k128():\n",
    "  hparams = transformer_base()\n",
    "  hparams.attention_key_channels = 128\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_k256():\n",
    "  hparams = transformer_base()\n",
    "  hparams.attention_key_channels = 256\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ff1024():\n",
    "  hparams = transformer_base()\n",
    "  hparams.filter_size = 1024\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ff4096():\n",
    "  hparams = transformer_base()\n",
    "  hparams.filter_size = 4096\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_dr0():\n",
    "  hparams = transformer_base()\n",
    "  hparams.layer_prepostprocess_dropout = 0.0\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_dr2():\n",
    "  hparams = transformer_base()\n",
    "  hparams.layer_prepostprocess_dropout = 0.2\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ls0():\n",
    "  hparams = transformer_base()\n",
    "  hparams.label_smoothing = 0.0\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ls2():\n",
    "  hparams = transformer_base()\n",
    "  hparams.label_smoothing = 0.2\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_hs256():\n",
    "  hparams = transformer_base()\n",
    "  hparams.hidden_size = 256\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_hs1024():\n",
    "  hparams = transformer_base()\n",
    "  hparams.hidden_size = 1024\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_big_dr1():\n",
    "  hparams = transformer_base()\n",
    "  hparams.hidden_size = 1024\n",
    "  hparams.filter_size = 4096\n",
    "  hparams.num_heads = 16\n",
    "  hparams.layer_prepostprocess_dropout = 0.1\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_big_enfr():\n",
    "  hparams = transformer_big_dr1()\n",
    "  hparams.shared_embedding_and_softmax_weights = False\n",
    "  hparams.filter_size = 8192\n",
    "  hparams.layer_prepostprocess_dropout = 0.1\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_big_enfr_tpu():\n",
    "  hparams = transformer_big_enfr()\n",
    "  # For performance, use fewer heads so that matrix dimensions are at least 128\n",
    "  hparams.num_heads = 8\n",
    "  update_hparams_for_tpu(hparams)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_big_dr2():\n",
    "  hparams = transformer_big_dr1()\n",
    "  hparams.layer_prepostprocess_dropout = 0.2\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_parameter_attention_a():\n",
    "  hparams = transformer_base()\n",
    "  hparams.ffn_layer = \"parameter_attention\"\n",
    "  hparams.filter_size = 1536\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_parameter_attention_b():\n",
    "  hparams = transformer_base()\n",
    "  hparams.ffn_layer = \"parameter_attention\"\n",
    "  hparams.filter_size = 512\n",
    "  hparams.parameter_attention_key_channels = 1024\n",
    "  hparams.parameter_attention_value_channels = 1024\n",
    "  hparams.num_heads = 16\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_prepend_v2():\n",
    "  hparams = transformer_base_v2()\n",
    "  hparams.prepend_mode = \"prepend_inputs_masked_attention\"\n",
    "  hparams.max_length = 0\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_prepend_v1():\n",
    "  hparams = transformer_base_v1()\n",
    "  hparams.prepend_mode = \"prepend_inputs_masked_attention\"\n",
    "  hparams.max_length = 0\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_prepend():\n",
    "  return transformer_prepend_v2()\n",
    "\n",
    "\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_base_range(rhp):\n",
    "  \"\"\"Small range of hyperparameters.\"\"\"\n",
    "  # After starting from base, set intervals for some parameters.\n",
    "  rhp.set_float(\"learning_rate\", 0.3, 3.0, scale=rhp.LOG_SCALE)\n",
    "  rhp.set_discrete(\"learning_rate_warmup_steps\",\n",
    "                   [1000, 2000, 4000, 8000, 16000])\n",
    "  rhp.set_float(\"initializer_gain\", 0.5, 2.0)\n",
    "  rhp.set_float(\"optimizer_adam_beta1\", 0.85, 0.95)\n",
    "  rhp.set_float(\"optimizer_adam_beta2\", 0.97, 0.99)\n",
    "  rhp.set_float(\"weight_decay\", 0.0, 1e-4)\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_relative():\n",
    "  \"\"\"Use relative position embeddings instead of absolute position encodings.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "  hparams.pos = None\n",
    "  hparams.self_attention_type = \"dot_product_relative\"\n",
    "  hparams.max_relative_position = 20\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_relative_tiny():\n",
    "  hparams = transformer_relative()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_relative_big():\n",
    "  hparams = transformer_big()\n",
    "  hparams.pos = None\n",
    "  hparams.self_attention_type = \"dot_product_relative\"\n",
    "  hparams.max_relative_position = 20\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_timeseries():\n",
    "  hparams = transformer_small()\n",
    "  hparams.batch_size = 256\n",
    "  hparams.learning_rate_warmup_steps = 2000\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_mlperf_tpu():\n",
    "  \"\"\"HParams for Transformer model on TPU for MLPerf on TPU 2x2.\"\"\"\n",
    "  hparams = transformer_base_v3()\n",
    "  hparams.mlperf_mode = True\n",
    "  hparams.symbol_modality_num_shards = 1\n",
    "  hparams.max_length = 256  # ignored when using \"_packed\" problems\n",
    "  hparams.batch_size = 2048  # per-chip batch size matches the reference model\n",
    "  hparams.hidden_size = 1024\n",
    "  hparams.filter_size = 4096\n",
    "  hparams.num_heads = 16\n",
    "  hparams.attention_dropout_broadcast_dims = \"0,1\"  # batch, heads\n",
    "  hparams.relu_dropout_broadcast_dims = \"1\"  # length\n",
    "  hparams.layer_prepostprocess_dropout_broadcast_dims = \"1\"  # length\n",
    "  return hparams\n",
    "\n",
    "\n",
    "def update_hparams_for_tpu(hparams):\n",
    "  \"\"\"Change hparams to be compatible with TPU training.\"\"\"\n",
    "\n",
    "  # Adafactor uses less memory than Adam.\n",
    "  # switch to Adafactor with its recommended learning rate scheme.\n",
    "  hparams.optimizer = \"Adafactor\"\n",
    "  hparams.learning_rate_schedule = \"rsqrt_decay\"\n",
    "  hparams.learning_rate_warmup_steps = 10000\n",
    "\n",
    "  # Avoid an expensive concat on TPU.\n",
    "  # >1 shards helps with faster parameter distribution on multi-GPU machines\n",
    "  hparams.symbol_modality_num_shards = 1\n",
    "\n",
    "  # Adaptive batch sizes and sequence lengths are not supported on TPU.\n",
    "  # Instead, every batch has the same sequence length and the same batch size.\n",
    "  # Longer sequences are dropped and shorter ones are padded.\n",
    "  #\n",
    "  # It is therefore suggested to use a problem where examples have been combined\n",
    "  # to a longer length, e.g. the \"_packed\" problems.\n",
    "  #\n",
    "  # For problems with variable sequence lengths, this parameter controls the\n",
    "  # maximum sequence length. Longer sequences are dropped and shorter ones\n",
    "  # are padded.\n",
    "  #\n",
    "  # For problems with fixed sequence lengths - e.g. the \"_packed\" problems,\n",
    "  # this hyperparameter is ignored.\n",
    "  hparams.max_length = 64\n",
    "\n",
    "  # TPUs have less memory than GPUs, so decrease the batch size if it's too high\n",
    "  if hparams.batch_size > 2048:\n",
    "    hparams.batch_size = 2048\n",
    "\n",
    "  # Using noise broadcast in the dropout layers saves memory during training.\n",
    "  hparams.attention_dropout_broadcast_dims = \"0,1\"  # batch, heads\n",
    "  hparams.relu_dropout_broadcast_dims = \"1\"  # length\n",
    "  hparams.layer_prepostprocess_dropout_broadcast_dims = \"1\"  # length\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tpu():\n",
    "  \"\"\"HParams for Transformer model on TPU.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_timeseries_tpu():\n",
    "  \"\"\"HParams for running Transformer model on timeseries on TPU.\"\"\"\n",
    "  hparams = transformer_timeseries()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "  hparams.batch_size = 256  # revert to value set in transformer_timeseries\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tpu_bf16_activation():\n",
    "  \"\"\"HParams for Transformer model with BF16 activation on TPU.\"\"\"\n",
    "  hparams = transformer_tpu()\n",
    "  hparams.activation_dtype = \"bfloat16\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_fairseq_fp16_activation_big():\n",
    "  \"\"\"Hparams intended to mirror those used in arxiv.org/pdf/1806.00187.pdf.\"\"\"\n",
    "  hparams = transformer_big()\n",
    "  hparams.activation_dtype = \"float16\"\n",
    "  hparams.batch_size = 3584\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_packed_tpu():\n",
    "  \"\"\"Deprecated alias for transformer_tpu().\"\"\"\n",
    "  return transformer_tpu()\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_big_tpu():\n",
    "  hparams = transformer_big()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tiny_tpu():\n",
    "  hparams = transformer_tiny()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_tiny_tpu_range(rhp):\n",
    "  \"\"\"Small range of hyperparameters.\"\"\"\n",
    "  rhp.set_float(\"learning_rate\", 0.3, 3.0, scale=rhp.LOG_SCALE)\n",
    "  rhp.set_float(\"weight_decay\", 0.0, 2.0)\n",
    "\n",
    "\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_tpu_range(rhp):\n",
    "  \"\"\"Small range of hyperparameters.\"\"\"\n",
    "  # After starting from base, set intervals for some parameters.\n",
    "  rhp.set_float(\"learning_rate\", 0.3, 3.0, scale=rhp.LOG_SCALE)\n",
    "  rhp.set_discrete(\"learning_rate_warmup_steps\",\n",
    "                   [1000, 2000, 4000, 8000, 16000])\n",
    "  rhp.set_float(\"initializer_gain\", 0.5, 2.0)\n",
    "  rhp.set_float(\"optimizer_adam_beta1\", 0.85, 0.95)\n",
    "  rhp.set_float(\"optimizer_adam_beta2\", 0.97, 0.99)\n",
    "  rhp.set_float(\"weight_decay\", 0.0, 2.0)\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_small_tpu():\n",
    "  \"\"\"TPU-friendly version of transformer_small.\n",
    "\n",
    "  Returns:\n",
    "    an hparams object.\n",
    "  \"\"\"\n",
    "  hparams = transformer_small()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_clean():\n",
    "  \"\"\"No dropout, label smoothing, max_length.\"\"\"\n",
    "  hparams = transformer_base_v2()\n",
    "  hparams.label_smoothing = 0.0\n",
    "  hparams.layer_prepostprocess_dropout = 0.0\n",
    "  hparams.attention_dropout = 0.0\n",
    "  hparams.relu_dropout = 0.0\n",
    "  hparams.max_length = 0\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_clean_big():\n",
    "  hparams = transformer_clean()\n",
    "  hparams.hidden_size = 1024\n",
    "  hparams.filter_size = 4096\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_clean_big_tpu():\n",
    "  hparams = transformer_clean_big()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tpu_with_conv():\n",
    "  \"\"\"Cut down on the number of heads, and use convs instead.\"\"\"\n",
    "  hparams = transformer_tpu()\n",
    "  hparams.num_heads = 4  # Heads are expensive on TPUs.\n",
    "  hparams.ffn_layer = \"conv_relu_conv\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_lm_tpu_0():\n",
    "  \"\"\"HParams for training languagemodel_lm1b8k on tpu.  92M Params.\"\"\"\n",
    "  hparams = transformer_clean_big()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "  hparams.num_heads = 4  # Heads are expensive on TPUs.\n",
    "  hparams.batch_size = 4096\n",
    "  hparams.shared_embedding_and_softmax_weights = False\n",
    "  hparams.layer_prepostprocess_dropout = 0.1\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_lm_tpu_1():\n",
    "  \"\"\"HParams for training languagemodel_lm1b8k on tpu.  335M Params.\"\"\"\n",
    "  hparams = transformer_lm_tpu_0()\n",
    "  hparams.hidden_size = 2048\n",
    "  hparams.filter_size = 8192\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_librispeech_v1():\n",
    "  \"\"\"HParams for training ASR model on LibriSpeech V1.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "\n",
    "  hparams.num_heads = 4\n",
    "  hparams.filter_size = 1024\n",
    "  hparams.hidden_size = 256\n",
    "  hparams.num_encoder_layers = 5\n",
    "  hparams.num_decoder_layers = 3\n",
    "  hparams.learning_rate = 0.15\n",
    "  hparams.batch_size = 6000000\n",
    "\n",
    "  librispeech.set_librispeech_length_hparams(hparams)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_librispeech_v2():\n",
    "  \"\"\"HParams for training ASR model on LibriSpeech V2.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "\n",
    "  hparams.max_length = 1240000\n",
    "  hparams.max_input_seq_length = 1550\n",
    "  hparams.max_target_seq_length = 350\n",
    "  hparams.batch_size = 16\n",
    "  hparams.num_decoder_layers = 4\n",
    "  hparams.num_encoder_layers = 6\n",
    "  hparams.hidden_size = 384\n",
    "  hparams.learning_rate = 0.15\n",
    "  hparams.daisy_chain_variables = False\n",
    "  hparams.filter_size = 1536\n",
    "  hparams.num_heads = 2\n",
    "  hparams.ffn_layer = \"conv_relu_conv\"\n",
    "  hparams.conv_first_kernel = 9\n",
    "  hparams.weight_decay = 0\n",
    "  hparams.layer_prepostprocess_dropout = 0.2\n",
    "  hparams.relu_dropout = 0.2\n",
    "\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_librispeech_tpu_v1():\n",
    "  \"\"\"HParams for training ASR model on Librispeech on TPU v1.\"\"\"\n",
    "  hparams = transformer_librispeech_v1()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "\n",
    "  hparams.batch_size = 16\n",
    "  librispeech.set_librispeech_length_hparams(hparams)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_librispeech_tpu_v2():\n",
    "  \"\"\"HParams for training ASR model on Librispeech on TPU v2.\"\"\"\n",
    "  hparams = transformer_librispeech_v2()\n",
    "  update_hparams_for_tpu(hparams)\n",
    "\n",
    "  hparams.batch_size = 16\n",
    "  librispeech.set_librispeech_length_hparams(hparams)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_librispeech_with_area_attention():\n",
    "  \"\"\"HParams for training ASR model on Librispeech on TPU v2.\"\"\"\n",
    "  hparams = transformer_librispeech_tpu_v2()\n",
    "  hparams.num_area_layers = 3  # area attn on first 3 encoder and decoder layers\n",
    "  hparams.max_area_width = 5\n",
    "  hparams.area_key_mode = \"concat\"\n",
    "  hparams.area_value_mode = \"sum\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_librispeech():\n",
    "  \"\"\"HParams for training ASR model on Librispeech.\"\"\"\n",
    "  return transformer_librispeech_v2()\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_librispeech_tpu():\n",
    "  \"\"\"HParams for training ASR model on Librispeech on TPU.\"\"\"\n",
    "  return transformer_librispeech_tpu_v2()\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_common_voice():\n",
    "  \"\"\"HParams for training ASR model on Mozilla Common Voice.\"\"\"\n",
    "  return transformer_librispeech()\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_common_voice_tpu():\n",
    "  \"\"\"HParams for training ASR model on Mozilla Common Voice on TPU.\"\"\"\n",
    "  hparams = transformer_librispeech_tpu()\n",
    "  hparams.batch_size = 8\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_supervised_attention():\n",
    "  \"\"\"HParams for supervised attention problems.\"\"\"\n",
    "  hparams = transformer_base()\n",
    "  # Attention loss type (KL-divergence or MSE).\n",
    "  hparams.add_hparam(\"expected_attention_loss_type\", \"kl_divergence\")\n",
    "  # Multiplier to the encoder-decoder expected attention loss.\n",
    "  hparams.add_hparam(\"expected_attention_loss_multiplier\", 1.0)\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_tpu_1b():\n",
    "    \"\"\"Hparams for machine translation with ~1.1B parameters.\"\"\"\n",
    "    hparams = transformer_tpu()\n",
    "    hparams.hidden_size = 2048\n",
    "    hparams.filter_size = 8192\n",
    "    hparams.num_hidden_layers = 8\n",
    "    # smaller batch size to avoid OOM\n",
    "    hparams.batch_size = 1024\n",
    "    hparams.activation_dtype = \"bfloat16\"\n",
    "    hparams.weight_dtype = \"bfloat16\"\n",
    "    # maximize number of parameters relative to computation by not sharing.\n",
    "    hparams.shared_embedding_and_softmax_weights = False\n",
    "    return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_wikitext103_l4k_v0():\n",
    "    \"\"\"HParams for training languagemodel_wikitext103_l4k.\"\"\"\n",
    "    hparams = transformer_big()\n",
    "    \n",
    "    # Adafactor uses less memory than Adam.\n",
    "    # switch to Adafactor with its recommended learning rate scheme.\n",
    "    hparams.optimizer = \"Adafactor\"\n",
    "    hparams.learning_rate_schedule = \"rsqrt_decay\"\n",
    "    hparams.learning_rate_warmup_steps = 10000\n",
    "    \n",
    "    hparams.num_heads = 4\n",
    "    hparams.max_length = 4096\n",
    "    hparams.batch_size = 4096\n",
    "    hparams.shared_embedding_and_softmax_weights = False\n",
    "    \n",
    "    hparams.num_hidden_layers = 8\n",
    "    hparams.attention_dropout = 0.1\n",
    "    hparams.layer_prepostprocess_dropout = 0.2\n",
    "    hparams.relu_dropout = 0.1\n",
    "    hparams.label_smoothing = 0.0\n",
    "    \n",
    "    # Using noise broadcast in the dropout layers saves memory during training.\n",
    "    hparams.attention_dropout_broadcast_dims = \"0,1\"  # batch, heads\n",
    "    hparams.relu_dropout_broadcast_dims = \"1\"  # length\n",
    "    hparams.layer_prepostprocess_dropout_broadcast_dims = \"1\"  # length\n",
    "    \n",
    "    # Avoid an expensive concat on TPU.\n",
    "    # >1 shards helps with faster parameter distribution on multi-GPU machines\n",
    "    hparams.symbol_modality_num_shards = 1\n",
    "    \n",
    "    return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_wikitext103_l4k_memory_v0():\n",
    "    \"\"\"HParams for training languagemodel_wikitext103_l4k with memory.\"\"\"\n",
    "    hparams = transformer_wikitext103_l4k_v0()\n",
    "    \n",
    "    hparams.split_targets_chunk_length = 64\n",
    "    hparams.split_targets_max_chunks = 64\n",
    "    hparams.split_targets_strided_training = True\n",
    "    hparams.add_hparam(\"memory_type\", \"transformer_xl\")\n",
    "    \n",
    "    # The hparams specify batch size *before* chunking, but we want to have a\n",
    "    # consistent 4K batch size *after* chunking to fully utilize the hardware.\n",
    "    target_tokens_per_batch = 4096\n",
    "    hparams.batch_size = int(target_tokens_per_batch * (\n",
    "      hparams.max_length / hparams.split_targets_chunk_length))  # 262144\n",
    "    \n",
    "    hparams.pos = None\n",
    "    hparams.self_attention_type = \"dot_product_relative\"\n",
    "    hparams.max_relative_position = 2 * hparams.split_targets_chunk_length\n",
    "    \n",
    "    hparams.add_hparam(\"unconditional\", True)\n",
    "    hparams.add_hparam(\"recurrent_memory_batch_size\", 0)  # 0 = try to guess\n",
    "    # By default, cache one chunk only (like Transformer-XL)\n",
    "    hparams.add_hparam(\"num_memory_items\", hparams.split_targets_chunk_length)\n",
    "    \n",
    "    return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_wikitext103_l16k_memory_v0():\n",
    "    \"\"\"HParams for training languagemodel_wikitext103_l16k with memory.\"\"\"\n",
    "    hparams = transformer_wikitext103_l4k_memory_v0()\n",
    "    \n",
    "    hparams.max_length = 16384\n",
    "    hparams.split_targets_chunk_length = 64\n",
    "    hparams.split_targets_max_chunks = int(\n",
    "      hparams.max_length / hparams.split_targets_chunk_length)\n",
    "    \n",
    "    # The hparams specify batch size *before* chunking, but we want to have a\n",
    "    # consistent 4K batch size *after* chunking to fully utilize the hardware.\n",
    "    target_tokens_per_batch = 4096\n",
    "    hparams.batch_size = int(target_tokens_per_batch * (\n",
    "      hparams.max_length / hparams.split_targets_chunk_length))\n",
    "    \n",
    "    hparams.max_relative_position = 2 * hparams.split_targets_chunk_length\n",
    "    \n",
    "    return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_cifar10_memory_v0():\n",
    "    \"\"\"HParams for training image_cifar10_plain_gen_flat_rev with memory.\"\"\"\n",
    "    hparams = transformer_wikitext103_l4k_memory_v0()\n",
    "    \n",
    "    hparams.num_hidden_layers = 6\n",
    "    \n",
    "    hparams.max_length = 32 * 32 * 3\n",
    "    hparams.split_targets_chunk_length = 64 * 3\n",
    "    hparams.split_targets_max_chunks = int(\n",
    "      hparams.max_length / hparams.split_targets_chunk_length)\n",
    "    hparams.num_memory_items = 128 * 3\n",
    "    \n",
    "    # Since this is an image problem, batch size refers to examples (not tokens)\n",
    "    target_images_per_batch = 4\n",
    "    hparams.batch_size = int(target_images_per_batch * (\n",
    "      hparams.max_length / hparams.split_targets_chunk_length))\n",
    "    \n",
    "    # The recurrent memory needs to know the actual batch size (in sequences)\n",
    "    hparams.recurrent_memory_batch_size = hparams.batch_size\n",
    "    \n",
    "    hparams.max_relative_position = (\n",
    "      hparams.num_memory_items + hparams.split_targets_chunk_length)\n",
    "    \n",
    "    return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_imagenet64_memory_v0():\n",
    "    \"\"\"HParams for training image_imagenet64_gen_flat_rev with memory.\"\"\"\n",
    "    hparams = transformer_cifar10_memory_v0()\n",
    "    \n",
    "    hparams.max_length = 64 * 64 * 3\n",
    "    hparams.split_targets_chunk_length = 64 * 3\n",
    "    hparams.split_targets_max_chunks = int(\n",
    "      hparams.max_length / hparams.split_targets_chunk_length)\n",
    "    hparams.num_memory_items = 128 * 3\n",
    "    \n",
    "    # Since this is an image problem, batch size refers to examples (not tokens)\n",
    "    target_images_per_batch = 2\n",
    "    hparams.batch_size = int(target_images_per_batch * (\n",
    "      hparams.max_length / hparams.split_targets_chunk_length))\n",
    "    \n",
    "    # The recurrent memory needs to know the actual batch size (in sequences)\n",
    "    hparams.recurrent_memory_batch_size = hparams.batch_size\n",
    "    \n",
    "    hparams.max_relative_position = 3072\n",
    "    \n",
    "    return hparams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
