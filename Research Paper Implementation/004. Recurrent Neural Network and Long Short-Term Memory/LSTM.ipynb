{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c53de8-4a0a-4fbe-b281-744c29ea3569",
   "metadata": {},
   "source": [
    "# 3. Constant Error Backprop\n",
    "## 3.1 Exponentially Decaying Error\n",
    "### Conventional BPTT\n",
    "Output  unit $k$'s target at time $t$ is denoted by $d_k(t)$.<br>\n",
    "Using mean squared error, $k$'s error signal is:<br>\n",
    "$$\n",
    "\\vartheta_k(t) = f^{\\prime} _k(net_k(t))(d_k(t)-y^k(t)),\n",
    "$$\n",
    "$$where$$\n",
    "$$\n",
    "y^i(t) = f_i(net_i(t))\n",
    "$$\n",
    "is the activation of a non-input unit $i$ with differentiable activation function $f_i$,\n",
    "$$\n",
    "net_i(t) = \\sum_j w_{ij}y^j(t-1)\n",
    "$$\n",
    "is unit$i$'s current net input, and $w_{ij}$ is the weight on the connection from unit $j$ to $i$.<br>\n",
    "Some non-output unit $j$'s backpropagated error signal is\n",
    "$$\n",
    "\\vartheta_j(t) = f^{\\prime}_j(net_j(t)) \\sum_i w_{ij} \\vartheta_i(t+1)\n",
    "$$\n",
    "The corresponding contribution to $w_{ji}$'s total weight update is $\\alpha \\vartheta_j(t) y^l(t-1)$,<br>\n",
    "where$\\alpha$ is the learning rate, and $l$ stands for an arbitrary unit connected to unit $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d26b0ff-5611-4814-b9d7-23b4af17860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_i(x, i):\n",
    "    return x[i] if x[i] > 0 else 0 # ReLU as an example\n",
    "\n",
    "def f_prime_i(x, i):\n",
    "    return 1 if x[i] > 0 else 0\n",
    "\n",
    "def net_i(t, weights, y, i):\n",
    "    return sum(weights[i][j] * y[j](t-1) for j in range(len(weights[i])))\n",
    "        \n",
    "def yi(t, net, i):\n",
    "    return f_i(net_i(t, net, y, i), i)\n",
    "    \n",
    "def vartheta_k(t, net, d, y, k):\n",
    "    return f_prime_i(net_i(t, net, y, k), k) * (d[k](t) - y[k](t))\n",
    "    \n",
    "def vartheta_j(t, net, weights, theta, j):\n",
    "    return f_prime_i(net_i(t, net, y, j), j) * sum(weights[i][j] * theta[i](t+1) for i in range(len(weights)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640ab8a-8f0b-47ee-86c9-e9de70833a9d",
   "metadata": {},
   "source": [
    "### Outline of Hchreiter's analysis.\n",
    "Suppose we have a fully connected net whose non-input unit indices range from 1 to $n$.<br>\n",
    "Let us focus on local error flow from unit $u$ to unit $v$ (later we will see that the analysis immediately extends to global error flow).<br>\n",
    "The error occuring at an arbitrary unit $u$ at time step $t$ is propagated \"Back into time\" for $q$ time steps, to an arbitrary unit $v$.<br>\n",
    "This will scale the error by the following factor:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\vartheta_v(t-q)}{\\partial \\vartheta_u(t)}=\\left\\{\\begin{array}{cc}\n",
    "f_v^{\\prime}\\left(\\operatorname{net}_v(t-1)\\right) w_{u v} & q=1 \\\\\n",
    "f_v^{\\prime}\\left(\\operatorname{net}_v(t-q)\\right) \\sum_{l=1}^n \\frac{\\partial \\vartheta_l(t-q+1)}{\\partial \\vartheta_u(t)} w_{l v} & q>1\n",
    "\\end{array} \\right.\n",
    "\\end{equation}\n",
    "With $l_q=v$ and $l_0 = u$, we obtain:\n",
    "$$\n",
    "\\frac{\\partial\\vartheta_v(t-q)}{\\partial\\vartheta_u(t)} = \\sum^n_{l1=1}...\\sum^n_{lq-1=1}\\prod^q_{m=1} f^{\\prime}_{l_m}(\\operatorname{net}_{l_m}(t-m))w_{l_ml_{m-1}}\n",
    "$$\n",
    "\n",
    "(proof by induction).<br>\n",
    "The sum of the $n^{q-1}$ terms $\\Pi^q_{m=1}f^{\\prime}_{l_m}(\\operatorname{net}_{l_m}(t-m))w_{l_ml_{m-1}}$ determines the total error back flow (note that since the summation terms may have different signs, increasing the number of units $n$ does not necessarily increase error flow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd88c3-a1ca-4cf6-90cc-349d03a20995",
   "metadata": {},
   "source": [
    "... I tried to implement all the equation on this chapter,<br>\n",
    "However, the Chapter 3 is about the problems the previous works have and too much to simply write it down.<br>\n",
    "I decided to understand them by reading the paper,<br>\n",
    "and focus on implementing LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a227d-7593-4ab9-8c82-cb6ebf2db83f",
   "metadata": {},
   "source": [
    "# 4. Long Short-Term Memory.\n",
    "## Memory Cells and gate units\n",
    "To construct an architecture that allows for constant error flow through special, self-connected units without disadvantages of the naive approach, we extend the constant error carousel (CEC) embodied by the self-connected, linear unit $j$ from section 3.2 by introducing additional features.<br>\n",
    "A multiplicative *input gate unit* is  introduced to protect the memory contents stored in $j$ from perturbation by irrelevant inputs.<br>\n",
    "Likewise, a multiplicative *output gate unit* is introduced which protects other units from perturbation by currently irrelevant memory contents stored in $j$.<br><br>\n",
    "The resulting, more complex unit is called a *memory cell*.<br>\n",
    "The $j$-th memory cell is denoted $c_j$.<br>\n",
    "Each memory cell is built around a central lineear unit with a fixed self-connection (the CEC).<br>\n",
    "In addition to $net_{c_j}$, $c_j$ gets input from multiplicative unit $out_j$ (the \"output gate\"), and from another multiplicative unit $in_j$ (the \"input gate\").<br>\n",
    "$in_j$'s activation at time $t$ is denoted by $y^{in_j}(t)$, $out_j$'s by $y^{out_j}(t)$.<br>\n",
    "We have:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    y^{out_j}(t) &= f_{out_j}(net_{out_j}(t)) \\\\\n",
    "    y^{in_j}(t) &= f_{in_j}(net_{in_j}(t))\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\\text{where}\n",
    "$$\n",
    "$$\n",
    "net_{out_j}(t) = \\sum_u w_{out_{j^u}}y^u(t-1),\n",
    "$$\n",
    "$$\n",
    "\\text{and}\n",
    "$$\n",
    "$$\n",
    "net_{in_j}(t) = \\sum_u w_{in_{j^u}}y^u(t-1),\n",
    "$$\n",
    "$$\n",
    "\\text{we also have}\n",
    "$$\n",
    "$$\n",
    "net_{c_j}(t) = \\sum_u w_{c_{j^u}}y^u(t-1),\n",
    "$$\n",
    "$$\n",
    "\\text{Jason's note: These all look same to me, which is:}\n",
    "$$\n",
    "$$\n",
    "y_i(t)= \\sum_u w_{i^u}y^u(t-1)\n",
    "$$\n",
    "The summation indices $u$ may stand for input units, gate units, memory cells, or even conventional hidden units if there are any.<br>\n",
    "All thes different types of units may convey useful information about the current state of the net.<br>\n",
    "For instance, an input gate may use inputs from other memory cells to decide whether to store certain information in its memory cell.<br>\n",
    "There even may be recurrent self-connections like $w_{{c_j}{c_j}}$.<br>\n",
    "It is up to the user to define the network topology.<br>\n",
    "At time $t$, $c_j$'s output $y^{c_j}(t)$ is computed as:\n",
    "$$\n",
    "y^{c_j}(t) = y^{out_j}(t)h(s_{c_j})(t)),\n",
    "$$\n",
    "$$\n",
    "\\text{where the \"internal state \"}s_{c_j}(t)\\text{ is}\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    s_{c_j}(0) &= 0, \\\\\n",
    "    s_{c_j}(t) &= s_{c_j}(t-1) + y^{in_j}(t)g(net_{c_j}(t)) for t > 0,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The differentiable function $g$ squashes $net_{c_j}$; the differentiable function $h$ scales memory cell outputs computed from the internal state $s_{c_j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adea421-8aed-4329-abb2-97fdef73b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_i(t, weights, prev_outputs):\n",
    "    '''\n",
    "    t : t\n",
    "    weights : w\n",
    "    prev_outputs : y\n",
    "    '''\n",
    "    net_input = np.dot(weights[i], prev_outputs)\n",
    "    return net_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7bb2bf-b243-46f1-a56b-f7cc853d4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryCell:\n",
    "    def __init__(self, input_units, cell_units):\n",
    "        self.w_in = np.random.randn(input_units, cell_units)\n",
    "        self.w_out = np.random.randn(input_units, cell_units)\n",
    "        self.w_c = np.random.randn(input_units, cell_units)\n",
    "        self.w_cc = np.random.randn(input_units, cell_units) # self-connection weight\n",
    "\n",
    "        self.s_c = np.zeros(cell_units)\n",
    "        self.y_in = np.zeros(cell_units)\n",
    "        self.y_out = np.zeros(cell_units)\n",
    "        self.y_c = np.zeros(cell_units)\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gate_activation(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def state_activation(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def net_input(self, y_previous, w):\n",
    "        # which is equivalent to the code in a cell above\n",
    "        return np.dot(y_previous, w)\n",
    "\n",
    "    def update(self, y_previous):\n",
    "        net_in = self.net_input(y_previous, self.w_in)\n",
    "        net_out = self.net_input(y_previous, self.w_out)\n",
    "        net_c = self.net_input(y_previous, self.w_c) + self.net_input(self.s_c, self.w_cc)\n",
    "\n",
    "        self.y_in = self.gate_activation(net_in)\n",
    "        self.y_out = self.gate_activation(net_out)\n",
    "        \n",
    "        self.s_c = self.s_c + self.y_in * self.state_activation(net_c)\n",
    "        self.y_c = self. y_out * self.state_activation(self.s_c)\n",
    "        return self.y_c        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70790584-34a6-43e2-9af4-1ecbec77909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "data = \"\"\"\n",
    "나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 따름이니라\n",
    "\"\"\"\n",
    "\n",
    "def preprocessing(text):\n",
    "    data = re.sub('[^가-힣]', ' ', text)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d4f1d-ac43-49e5-b829-574e1027a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def sigmoid_derivative(input):\n",
    "    return input * (1 - input)\n",
    "\n",
    "def tanh(input, derivative=False):\n",
    "    return np.tanh(input)\n",
    "\n",
    "def tanh_derivative(input):\n",
    "    return 1 - input ** 2\n",
    "\n",
    "def so"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
