{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfdf246-5d97-4c84-894d-b359ac4b9110",
   "metadata": {},
   "source": [
    "# RNN Training on 훈민정음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b130a0-e60c-44b5-bd43-89622a193966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73a7f52-59b8-4e54-bf83-b0a08b294416",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 따름이니라\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9d3a2f-9e79-4fb8-8d5a-30873c5f9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    data = re.sub('[^가-힣]', ' ', text)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "985b5223-de46-4899-b763-95df3daea70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나라의', '말이', '중국과', '달라', '문자와', '서로', '통하지', '아니하기에', '이런', '까닭으로', '어리석은', '백성이', '이르고자', '할', '바가', '있어도', '마침내', '제', '뜻을', '능히', '펴지', '못할', '사람이', '많으니라', '내가', '이를', '위해', '가엾이', '여겨', '새로', '스물여덟', '글자를', '만드노니', '사람마다', '하여', '쉬이', '익혀', '날로', '씀에', '편안케', '하고자', '할', '따름이니라']\n",
      "\n",
      "42\n",
      "\n",
      "{'가엾이': 0, '글자를': 1, '씀에': 2, '어리석은': 3, '백성이': 4, '이르고자': 5, '내가': 6, '바가': 7, '많으니라': 8, '여겨': 9, '서로': 10, '하여': 11, '편안케': 12, '능히': 13, '만드노니': 14, '쉬이': 15, '할': 16, '스물여덟': 17, '하고자': 18, '말이': 19, '까닭으로': 20, '사람마다': 21, '익혀': 22, '이런': 23, '펴지': 24, '따름이니라': 25, '있어도': 26, '위해': 27, '문자와': 28, '아니하기에': 29, '마침내': 30, '뜻을': 31, '새로': 32, '제': 33, '나라의': 34, '날로': 35, '중국과': 36, '통하지': 37, '못할': 38, '달라': 39, '사람이': 40, '이를': 41}\n",
      "\n",
      "{0: '가엾이', 1: '글자를', 2: '씀에', 3: '어리석은', 4: '백성이', 5: '이르고자', 6: '내가', 7: '바가', 8: '많으니라', 9: '여겨', 10: '서로', 11: '하여', 12: '편안케', 13: '능히', 14: '만드노니', 15: '쉬이', 16: '할', 17: '스물여덟', 18: '하고자', 19: '말이', 20: '까닭으로', 21: '사람마다', 22: '익혀', 23: '이런', 24: '펴지', 25: '따름이니라', 26: '있어도', 27: '위해', 28: '문자와', 29: '아니하기에', 30: '마침내', 31: '뜻을', 32: '새로', 33: '제', 34: '나라의', 35: '날로', 36: '중국과', 37: '통하지', 38: '못할', 39: '달라', 40: '사람이', 41: '이를'}\n"
     ]
    }
   ],
   "source": [
    "tokens, vocab_size, word_to_idx, idx_to_word = preprocessing(data)\n",
    "print(tokens)\n",
    "print()\n",
    "print(vocab_size)\n",
    "print()\n",
    "print(word_to_idx)\n",
    "print()\n",
    "print(idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46612302-df0b-4e6b-bfbc-501fe5665f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(h_size, vocab_size):\n",
    "    U = np.random.randn(h_size, vocab_size) * 0.01\n",
    "    W = np.random.randn(h_size, h_size) * 0.01\n",
    "    V = np.random.randn(vocab_size, h_size) * 0.01\n",
    "    return U, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06441d0a-4f51-418a-be72-37b4742faf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(inputs, targets, hprev):\n",
    "    loss = 0\n",
    "    xs, hs, ps, ys = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for i in range(seq_len):\n",
    "        xs[i] = np.zeros((vocab_size, 1))\n",
    "        xs[i][inputs[i]] = 1 # one-hot encoding\n",
    "        hs[i] = np.tanh(np.dot(U,xs[i]) + np.dot(W, hs[i-1])) # hidden_state\n",
    "        ys[i] = np.dot(V, hs[i]) # o_1 = Vh_1\n",
    "        ps[i] = np.exp(ys[i]) / np.sum(np.exp(ys[i])) # softmax\n",
    "        loss += -np.log(ps[i][targets[i], 0]) # cross-entropy\n",
    "    return loss, ps, hs, xs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b652e543-12e3-4cf3-8a57-d28a55637f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(ps, hs, xs):\n",
    "    # backpropagation through time ## BPTT\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    dU = np.zeros(U.shape)\n",
    "\n",
    "    for i in range(seq_len)[::-1]:\n",
    "        output = np.zeros((vocab_size, 1))\n",
    "        output[targets[i]] = 1\n",
    "        ps[i] = ps[i] - output.reshape(-1, 1)\n",
    "        # calculating dL/dvi at each step i\n",
    "        dV_step_i = ps[i] @ (hs[i]).T # (y_hat - y) @ hs.T\n",
    "\n",
    "        dV = dV + dV_step_i # add all dv/dVi\n",
    "        \n",
    "        # in order to calculate V and W per each i\n",
    "        # it is better to calculate the common part and save it as delta\n",
    "        # Then get dL/dWij and dL/dUij\n",
    "        # get dL/dW and dL/dU by adding up each of them\n",
    "        # then update delta with it.\n",
    "\n",
    "        # delta that will be used in the loop\n",
    "        delta_recent = (V.T @ ps[i]) * (1 - hs[i] ** 2) # (y_hat - y) * V * (1 - h_{i-1}^2)\n",
    "\n",
    "        for j in range(i + 1)[::-1]:\n",
    "            dW_ij = delta_recent @ hs[j-1].T\n",
    "            dW = dW + dW_ij\n",
    "            dU_ij = delta_recent @ xs[j].reshape(1, -1)\n",
    "            dU = dU + dU_ij\n",
    "            \n",
    "            # updating delta to be used in the next loop.\n",
    "            delta_recent = (W.T @ delta_recent) * (1-hs[j-1]**2)\n",
    "        for d in [dU, dW, dV]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "    return dU, dW, dV, hs[len(inputs) - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "529466d6-1ab7-4eea-8caf-ecee98b964ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word, length):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[word_to_idx[word]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((h_size, 1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(U, x) + np.dot(W, h))\n",
    "        y = np.dot(V, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))    # softmax\n",
    "        idx = np.argmax(p)                   # return the index that has the highest probability\n",
    "        x = np.zeros((vocab_size, 1))        # prepare the next input\n",
    "        x[idx] = 1\n",
    "        ixes.append(idx)\n",
    "    pred_words = ' '.join(idx_to_word[i] for i in ixes)\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f9a7023-baf6-4296-86d4-e6f422f2c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "h_size = 1000\n",
    "seq_len = 3\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a249ee26-c2c1-4467-9850-ffc0aabf480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, W, V = init_weights(h_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57f27772-f6b6-409d-85ac-cf77fa5862d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed08920a52024bb78ae9d6f67e6b87af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 11.120202059885667\n",
      "epoch 100, loss: 1.0539873788899932\n",
      "epoch 200, loss: 0.08152424355107796\n",
      "epoch 300, loss: 0.04898888026406524\n",
      "epoch 400, loss: 0.02728792125964602\n",
      "epoch 500, loss: 0.018152832688487996\n",
      "epoch 600, loss: 0.012772458748214446\n",
      "epoch 700, loss: 0.009398687458355058\n",
      "epoch 800, loss: 0.007171430400222013\n",
      "epoch 900, loss: 0.005614459393640043\n",
      "epoch 1000, loss: 0.004496547313407035\n",
      "epoch 1100, loss: 0.0037082819183412493\n",
      "epoch 1200, loss: 0.003169889561582531\n",
      "epoch 1300, loss: 0.0028272919007194474\n",
      "epoch 1400, loss: 0.0026402149430549394\n",
      "epoch 1500, loss: 0.002558961380749707\n",
      "epoch 1600, loss: 0.0025224168161854703\n",
      "epoch 1700, loss: 0.002476034191083159\n",
      "epoch 1800, loss: 0.002381518376694911\n",
      "epoch 1900, loss: 0.0022327983054101544\n",
      "epoch 2000, loss: 0.0020584587873681962\n",
      "epoch 2100, loss: 0.0018909908442846513\n",
      "epoch 2200, loss: 0.0017451348416219412\n",
      "epoch 2300, loss: 0.0016218391617572497\n",
      "epoch 2400, loss: 0.0015172569677594679\n",
      "epoch 2500, loss: 0.001427024639098021\n",
      "epoch 2600, loss: 0.0013474447442723908\n",
      "epoch 2700, loss: 0.001275765850412734\n",
      "epoch 2800, loss: 0.00121018061621756\n",
      "epoch 2900, loss: 0.0011496284393739014\n",
      "epoch 3000, loss: 0.0010935202014368498\n",
      "epoch 3100, loss: 0.0010415040817289365\n",
      "epoch 3200, loss: 0.000993318538292065\n",
      "epoch 3300, loss: 0.0009487213950445914\n",
      "epoch 3400, loss: 0.0009074660385100081\n",
      "epoch 3500, loss: 0.0008693001366216699\n",
      "epoch 3600, loss: 0.0008339720902199833\n",
      "epoch 3700, loss: 0.000801238206115992\n",
      "epoch 3800, loss: 0.0007708680830146975\n",
      "epoch 3900, loss: 0.0007426478078612841\n",
      "epoch 4000, loss: 0.000716381337126978\n",
      "epoch 4100, loss: 0.0006918906028558046\n",
      "epoch 4200, loss: 0.000669014818565513\n",
      "epoch 4300, loss: 0.0006476093379865933\n",
      "epoch 4400, loss: 0.000627544305676422\n",
      "epoch 4500, loss: 0.0006087032509789048\n",
      "epoch 4600, loss: 0.0005909817154404558\n",
      "epoch 4700, loss: 0.0005742859632031612\n",
      "epoch 4800, loss: 0.0005585317981539149\n",
      "epoch 4900, loss: 0.0005436434958462977\n",
      "epoch 5000, loss: 0.0005295528490433409\n",
      "epoch 5100, loss: 0.0005161983206565534\n",
      "epoch 5200, loss: 0.0005035242953756166\n",
      "epoch 5300, loss: 0.000491480420328523\n",
      "epoch 5400, loss: 0.0004800210251034308\n",
      "epoch 5500, loss: 0.00046910461190472993\n",
      "epoch 5600, loss: 0.0004586934073431729\n",
      "epoch 5700, loss: 0.00044875296820258035\n",
      "epoch 5800, loss: 0.00043925183433974355\n",
      "epoch 5900, loss: 0.0004301612227006137\n",
      "epoch 6000, loss: 0.0004214547571806861\n",
      "epoch 6100, loss: 0.0004131082297283372\n",
      "epoch 6200, loss: 0.00040509938869722785\n",
      "epoch 6300, loss: 0.0003974077509890958\n",
      "epoch 6400, loss: 0.00039001443498224017\n",
      "epoch 6500, loss: 0.00038290201165894954\n",
      "epoch 6600, loss: 0.00037605437168142654\n",
      "epoch 6700, loss: 0.0003694566064797043\n",
      "epoch 6800, loss: 0.00036309490166790126\n",
      "epoch 6900, loss: 0.0003569564413270041\n",
      "epoch 7000, loss: 0.00035102932190354125\n",
      "epoch 7100, loss: 0.00034530247460449104\n",
      "epoch 7200, loss: 0.00033976559534615975\n",
      "epoch 7300, loss: 0.00033440908142129595\n",
      "epoch 7400, loss: 0.00032922397414324547\n",
      "epoch 7500, loss: 0.00032420190684124336\n",
      "epoch 7600, loss: 0.0003193350576337088\n",
      "epoch 7700, loss: 0.00031461610649837987\n",
      "epoch 7800, loss: 0.00031003819618067837\n",
      "epoch 7900, loss: 0.00030559489658665167\n",
      "epoch 8000, loss: 0.000301280172287417\n",
      "epoch 8100, loss: 0.00029708835284843665\n",
      "epoch 8200, loss: 0.00029301410570495756\n",
      "epoch 8300, loss: 0.00028905241135092824\n",
      "epoch 8400, loss: 0.00028519854061348916\n",
      "epoch 8500, loss: 0.00028144803382577894\n",
      "epoch 8600, loss: 0.0002777966817232424\n",
      "epoch 8700, loss: 0.0002742405079038415\n",
      "epoch 8800, loss: 0.0002707757527177839\n",
      "epoch 8900, loss: 0.0002673988584448336\n",
      "epoch 9000, loss: 0.00026410645566357725\n",
      "epoch 9100, loss: 0.0002608953506949238\n",
      "epoch 9200, loss: 0.0002577625140267705\n",
      "epoch 9300, loss: 0.000254705069640537\n",
      "epoch 9400, loss: 0.00025172028515527595\n",
      "epoch 9500, loss: 0.0002488055627269439\n",
      "epoch 9600, loss: 0.00024595843062554006\n",
      "epoch 9700, loss: 0.00024317653545245996\n",
      "epoch 9800, loss: 0.00024045763492465974\n",
      "epoch 9900, loss: 0.0002377995911883157\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "p = 0\n",
    "hprev = np.zeros((h_size, 1))\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for p in range(len(tokens) - seq_len):\n",
    "        inputs = [word_to_idx[token] for token in tokens[p:p + seq_len]]\n",
    "        targets = [word_to_idx[token] for token in tokens[p+1:p + seq_len+1]]\n",
    "\n",
    "        loss, ps, hs, xs = feedforward(inputs, targets, hprev)\n",
    "\n",
    "        dU, dW, dV, hprev = backward(ps, hs, xs)\n",
    "        \n",
    "        # update weights and biases using gradient descent\n",
    "        W -= learning_rate * dW\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "        # p += seq_len\n",
    "\n",
    "    if epoch %100 == 0:\n",
    "        print(f\"epoch {epoch}, loss: {loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8584d8c5-031e-47f5-9982-24f3c9d374bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input:  나라의\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말이 중국과 달라 문자와 달라 문자와 서로 문자와 서로 통하지 서로 통하지 아니하기에 통하지 아니하기에 이런 아니하기에 이런 까닭으로 이런 까닭으로 어리석은 까닭으로 어리석은 백성이 어리석은 백성이 이르고자 백성이 이르고자 할 이르고자 할 바가 할 바가 있어도 바가 있어도 마침내\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input:  break\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    try:\n",
    "        user_input = input('input: ')\n",
    "        if user_input == 'break':\n",
    "            break\n",
    "        response = predict(user_input, 40)\n",
    "        print(response)\n",
    "    except:\n",
    "        print('Try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ca176-9580-4d0a-b139-29f08e8653be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
