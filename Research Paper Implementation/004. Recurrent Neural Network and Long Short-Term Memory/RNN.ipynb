{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfdf246-5d97-4c84-894d-b359ac4b9110",
   "metadata": {},
   "source": [
    "# RNN Training on 훈민정음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b130a0-e60c-44b5-bd43-89622a193966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73a7f52-59b8-4e54-bf83-b0a08b294416",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 따름이니라\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9d3a2f-9e79-4fb8-8d5a-30873c5f9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    data = re.sub('[^가-힣]', ' ', text)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "985b5223-de46-4899-b763-95df3daea70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나라의', '말이', '중국과', '달라', '문자와', '서로', '통하지', '아니하기에', '이런', '까닭으로', '어리석은', '백성이', '이르고자', '할', '바가', '있어도', '마침내', '제', '뜻을', '능히', '펴지', '못할', '사람이', '많으니라', '내가', '이를', '위해', '가엾이', '여겨', '새로', '스물여덟', '글자를', '만드노니', '사람마다', '하여', '쉬이', '익혀', '날로', '씀에', '편안케', '하고자', '할', '따름이니라']\n",
      "\n",
      "42\n",
      "\n",
      "{'가엾이': 0, '글자를': 1, '씀에': 2, '어리석은': 3, '백성이': 4, '이르고자': 5, '내가': 6, '바가': 7, '많으니라': 8, '여겨': 9, '서로': 10, '하여': 11, '편안케': 12, '능히': 13, '만드노니': 14, '쉬이': 15, '할': 16, '스물여덟': 17, '하고자': 18, '말이': 19, '까닭으로': 20, '사람마다': 21, '익혀': 22, '이런': 23, '펴지': 24, '따름이니라': 25, '있어도': 26, '위해': 27, '문자와': 28, '아니하기에': 29, '마침내': 30, '뜻을': 31, '새로': 32, '제': 33, '나라의': 34, '날로': 35, '중국과': 36, '통하지': 37, '못할': 38, '달라': 39, '사람이': 40, '이를': 41}\n",
      "\n",
      "{0: '가엾이', 1: '글자를', 2: '씀에', 3: '어리석은', 4: '백성이', 5: '이르고자', 6: '내가', 7: '바가', 8: '많으니라', 9: '여겨', 10: '서로', 11: '하여', 12: '편안케', 13: '능히', 14: '만드노니', 15: '쉬이', 16: '할', 17: '스물여덟', 18: '하고자', 19: '말이', 20: '까닭으로', 21: '사람마다', 22: '익혀', 23: '이런', 24: '펴지', 25: '따름이니라', 26: '있어도', 27: '위해', 28: '문자와', 29: '아니하기에', 30: '마침내', 31: '뜻을', 32: '새로', 33: '제', 34: '나라의', 35: '날로', 36: '중국과', 37: '통하지', 38: '못할', 39: '달라', 40: '사람이', 41: '이를'}\n"
     ]
    }
   ],
   "source": [
    "tokens, vocab_size, word_to_idx, idx_to_word = preprocessing(data)\n",
    "print(tokens)\n",
    "print()\n",
    "print(vocab_size)\n",
    "print()\n",
    "print(word_to_idx)\n",
    "print()\n",
    "print(idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46612302-df0b-4e6b-bfbc-501fe5665f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(h_size, vocab_size):\n",
    "    U = np.random.randn(h_size, vocab_size) * 0.01\n",
    "    W = np.random.randn(h_size, h_size) * 0.01\n",
    "    V = np.random.randn(vocab_size, h_size) * 0.01\n",
    "    return U, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06441d0a-4f51-418a-be72-37b4742faf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(inputs, targets, hprev):\n",
    "    loss = 0\n",
    "    xs, hs, ps, ys = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for i in range(seq_len):\n",
    "        xs[i] = np.zeros((vocab_size, 1))\n",
    "        xs[i][inputs[i]] = 1 # one-hot encoding\n",
    "        hs[i] = np.tanh(np.dot(U,xs[i]) + np.dot(W, hs[i-1])) # hidden_state\n",
    "        ys[i] = np.dot(V, hs[i]) # o_1 = Vh_1\n",
    "        ps[i] = np.exp(ys[i]) / np.sum(np.exp(ys[i])) # softmax\n",
    "        loss += -np.log(ps[i][targets[i], 0]) # cross-entropy\n",
    "    return loss, ps, hs, xs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b652e543-12e3-4cf3-8a57-d28a55637f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(ps, hs, xs):\n",
    "    # backpropagation through time ## BPTT\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    dU = np.zeros(U.shape)\n",
    "\n",
    "    for i in range(seq_len)[::-1]:\n",
    "        output = np.zeros((vocab_size, 1))\n",
    "        output[targets[i]] = 1\n",
    "        ps[i] = ps[i] - output.reshape(-1, 1)\n",
    "        # calculating dL/dvi at each step i\n",
    "        dV_step_i = ps[i] @ (hs[i]).T # (y_hat - y) @ hs.T\n",
    "\n",
    "        dV = dV + dV_step_i # add all dv/dVi\n",
    "        \n",
    "        # in order to calculate V and W per each i\n",
    "        # it is better to calculate the common part and save it as delta\n",
    "        # Then get dL/dWij and dL/dUij\n",
    "        # get dL/dW and dL/dU by adding up each of them\n",
    "        # then update delta with it.\n",
    "\n",
    "        # delta that will be used in the loop\n",
    "        delta_recent = (V.T @ ps[i]) * (1 - hs[i] ** 2) # (y_hat - y) * V * (1 - h_{i-1}^2)\n",
    "\n",
    "        for j in range(i + 1)[::-1]:\n",
    "            dW_ij = delta_recent @ hs[j-1].T\n",
    "            dW = dW + dW_ij\n",
    "            dU_ij = delta_recent @ xs[j].reshape(1, -1)\n",
    "            dU = dU + dU_ij\n",
    "            \n",
    "            # updating delta to be used in the next loop.\n",
    "            delta_recent = (W.T @ delta_recent) * (1-hs[j-1]**2)\n",
    "        for d in [dU, dW, dV]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "    return dU, dW, dV, hs[len(inputs) - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "529466d6-1ab7-4eea-8caf-ecee98b964ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word, length):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[word_to_idx[word]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((h_size, 1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(U, x) + np.dot(W, h))\n",
    "        y = np.dot(V, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))    # softmax\n",
    "        idx = np.argmax(p)                   # return the index that has the highest probability\n",
    "        x = np.zeros((vocab_size, 1))        # prepare the next input\n",
    "        x[idx] = 1\n",
    "        ixes.append(idx)\n",
    "    pred_words = ' '.join(idx_to_word[i] for i in ixes)\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f9a7023-baf6-4296-86d4-e6f422f2c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "h_size = 1000\n",
    "seq_len = 3\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a249ee26-c2c1-4467-9850-ffc0aabf480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, W, V = init_weights(h_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f27772-f6b6-409d-85ac-cf77fa5862d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed08920a52024bb78ae9d6f67e6b87af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 11.120202059885667\n",
      "epoch 100, loss: 1.0539873788899932\n",
      "epoch 200, loss: 0.08152424355107796\n",
      "epoch 300, loss: 0.04898888026406524\n",
      "epoch 400, loss: 0.02728792125964602\n",
      "epoch 500, loss: 0.018152832688487996\n",
      "epoch 600, loss: 0.012772458748214446\n",
      "epoch 700, loss: 0.009398687458355058\n",
      "epoch 800, loss: 0.007171430400222013\n",
      "epoch 900, loss: 0.005614459393640043\n",
      "epoch 1000, loss: 0.004496547313407035\n",
      "epoch 1100, loss: 0.0037082819183412493\n",
      "epoch 1200, loss: 0.003169889561582531\n",
      "epoch 1300, loss: 0.0028272919007194474\n",
      "epoch 1400, loss: 0.0026402149430549394\n",
      "epoch 1500, loss: 0.002558961380749707\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "p = 0\n",
    "hprev = np.zeros((h_size, 1))\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for p in range(len(tokens) - seq_len):\n",
    "        inputs = [word_to_idx[token] for token in tokens[p:p + seq_len]]\n",
    "        targets = [word_to_idx[token] for token in tokens[p+1:p + seq_len+1]]\n",
    "\n",
    "        loss, ps, hs, xs = feedforward(inputs, targets, hprev)\n",
    "\n",
    "        dU, dW, dV, hprev = backward(ps, hs, xs)\n",
    "        \n",
    "        # update weights and biases using gradient descent\n",
    "        W -= learning_rate * dW\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "        # p += seq_len\n",
    "\n",
    "    if epoch %100 == 0:\n",
    "        print(f\"epoch {epoch}, loss: {loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584d8c5-031e-47f5-9982-24f3c9d374bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
