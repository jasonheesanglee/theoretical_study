{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6b359f-320c-4ae5-9dc9-247988ef13a4",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0adbde6-f422-49b3-870e-649712d3c65b",
   "metadata": {},
   "source": [
    "The simplest form of the learning procedure is for layered networks,<br> which have a layer of input units at the bottom;<br> any number of intermediate layers;<br> and a layer of output units at the top.<br><br>\n",
    "Connections within a layer or from higher to lower layers are forbidden, but the connections **can skip intermediate layers**.<br>\n",
    "An input vector is presented to the network by setting the states of the input units.<br>\n",
    "Then the states of the units in each layer are determined by applying equations (1) and (2) to the connections coming from the lower layers.<br>\n",
    "All units within a layer have their states set in parallel, but different layers have their states set sequentially, starting at the bottom and working upwards until the states of the output units are determined.<br><br>\n",
    "\n",
    "## Equation (1).\n",
    "The total input $x_j$, to unit $j$ is a linear function of the outputs $y_i$, of the units that are connected to $j$ and of the weights $w_{ji}$, on these connections.\n",
    "$$\n",
    "x_j = \\sum_i y_iw_{ji}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143d9348-ae91-4829-8377-645725ed27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_j(y, w):\n",
    "    y_w_multi = []\n",
    "    for y_i, w_ji in zip(y, w):\n",
    "        y_w_multi.append(y_i * w_ji)\n",
    "    x_j = sum(y_w_multi)\n",
    "    return x_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894f0c5-960b-425a-bddc-4a39b68f7e37",
   "metadata": {},
   "source": [
    "## Equation (2).\n",
    "Units can be given biases by introducing an extra input to each unit which always has a value of 1.<br>\n",
    "The weight on this extra input is called the bias and is equivalent to a threshold of the opposite sign.<br>\n",
    "It can be treated just like the other weights.<br>\n",
    "A unit has a real-valued output $y_j$, which is a non-linear function of its total input.\n",
    "$$\n",
    "y_j = \\frac{1}{1+e^{-x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75b8681-f505-428b-af35-4379462c83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid_activation(x):\n",
    "    return 1/(1+math.exp**(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35eba5-5e6b-41ca-9a5f-c1cda5bba5c2",
   "metadata": {},
   "source": [
    "It is not necessary to use exactly the functions given in equations (1) and (2).<br>\n",
    "Any input-output function which has a bounded derivative will do.<br><br>\n",
    "However, the use of a linear function for combining the inputs to a unit before applying the nonlinearity greatly simplifies the learning procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00b8b5-9567-4e2b-9328-94485ff53fe0",
   "metadata": {},
   "source": [
    "# Equation (3)\n",
    "The aim is to find a set of weights that ensure that for each input vector the output vector produced by the network is the same as (or sufficiently close to) the desired output vector.<br>\n",
    "If there is a fixed, finite set of input-output cases, the total error in the performance of the network with a particular set of weights can be computed by comparing the actual and desired output vectors for every case.<br><br>\n",
    "The total error $E$ is definded as:\n",
    "$$\n",
    "E = \\frac{1}{2}\\sum_c\\sum_j(y_{j,c}-d_{j,c})^2\n",
    "$$\n",
    "$where$<br>\n",
    "$c$ is an index over cases (input-output pairs),<br>\n",
    "$j$ is an index over output units,<br>\n",
    "$y$ is the actual state of an output unit,<br>\n",
    "$d$ is its desired state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d23812e-3ac2-45e8-9a40-5e181b28e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_error(Y, D):\n",
    "    E = 0\n",
    "    for y_c, d_c in zip(Y, D):\n",
    "        yc = []\n",
    "        for y_j, d_j in zip(y_c, d_j):\n",
    "            yc.append((y_j - d_j)**2)\n",
    "        E = E + sum(yc)\n",
    "    E = 0.5 * E\n",
    "    return E            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7478f9-4180-40c6-8e59-4662bdbcbb1a",
   "metadata": {},
   "source": [
    "# Equation (4)\n",
    "\n",
    "The backward pass starts by computing $\\partial E / \\partial y$ for each of the output units.<br>\n",
    "Differentiating equation(3) for a particular case $c$, and suppressing the index $c$ gives.\n",
    "$$\n",
    "\\partial E / \\partial y_j = y_j - d_j\n",
    "$$\n",
    "We can then apply the chain rule to compute $\\partial E / \\partial x_j$\n",
    "$$\n",
    "\\partial E / \\partial x_j = \\partial E / \\partial y_j \\cdot dy_j / dx_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09ea6db-e3e1-4043-bf56-a2482ba31642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(Y, D, dy_dx):\n",
    "    dE_dy = [y_j - d_j for y_j, d_j in zip(Y, D)]\n",
    "    dE_dx = [dE_dy_j * dy_dx_j for dE_dy_j, dy_dx,j in zip(dE_dy, dy_dx)]\n",
    "    return dE_dy, dE_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884690cf-014a-4248-b71a-51d20f0b2f96",
   "metadata": {},
   "source": [
    "# Equation (5)\n",
    "Differentiating equation(2) to get the value of $dy_j/dx_j$ and substituting gives:\n",
    "$$\n",
    "\\partial E / \\partial x_j = \\partial E / \\partial y_j \\cdot y_j(1-y_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63171d20-1654-4ddc-b2cc-94e15aa4af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dE_dx(Y, dE_dy):\n",
    "    dE_dx = [dE_dy_j * y_j * (1 - y_j) for dE_dy_j, y_j in zip(dE_dy, Y)]\n",
    "    return dE_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b331c-3cfc-4d5c-9cee-ea76a63903dd",
   "metadata": {},
   "source": [
    "# Equation(6)\n",
    "This means that we know how a change in the total input $x$ to an output unit will affect the error.<br>\n",
    "But this total input is just a linear function of the states of the lower level units and it is also a linear function of the weights on the connections,<br>so it is easy to comput how the error will be affected by changing these states and weights.<br>\n",
    "For a weight $w_{ji}$ from $i$ to $j$ the derivative is:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    \\partial E / \\partial w_{ji} &= \\partial E / \\partial x_j \\cdot \\partial x_j / \\partial w_{ji}\\\\\n",
    "    &=\\partial E / \\partial x_j \\cdot y_i\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "and for the output of the $i^{th}$ unit the contribution to $\\partial E / \\partial y_i$ resulting from the effect of $i$ on $j$ is simply\n",
    "$$\n",
    "\\partial E / \\partial x_j \\cdot \\partial x_j / \\partial y_i = \\partial E / \\partial x_j \\cdot w_{ji}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c82b1a7-50ce-4cab-8999-0b76241e2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight_and_output_gradients(dE_dx, Y, W):\n",
    "    dE_dw = [[dE_dx_j * y_i for y_i in Y] for dE_dx_j in dE_dx]\n",
    "    dE_dy = [sum(dE_dx_j * w_ji for dE_dx_j, w_ji in zip(dE_dx, w_row)) for w_row in W]\n",
    "\n",
    "    return dE_dw, dE_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1a652-06b1-4e17-baa2-88565a0810d6",
   "metadata": {},
   "source": [
    "# Equation (7)\n",
    "so taking into account all the connections emanating from unit $i$ we have\n",
    "$$\n",
    "\\partial E / \\partial y_i = \\sum_j \\partial E/\\partial x_j \\cdot w_{ji}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c399a84-c4f6-46cf-a4d7-f6a2e2ca77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_dE_dy(dE_dx, W):\n",
    "    dE_dy = [sum(dE_dx_j * w_row[i] for dE_dx_j, w_row in zip(dE_dx, W)) for i in range(len(W[0]))]\n",
    "\n",
    "    return dE_dy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19baa5e9-3fb1-44ed-9dfb-18978e1261c1",
   "metadata": {},
   "source": [
    "# Equation (8)\n",
    "One way of using $\\partial E / \\partial w$ is to change the weights after every input-output case.<br>\n",
    "This has the advantage that no separate memory is required for the derivatives.<br>\n",
    "An alternative scheme, which we used in the research reported here, is to accumulate $\\partial E / \\partial w$ over all the input-output cases before changing the weights.<br>\n",
    "The simplest version of gradient descent is to change each weight by an amount of proportional to the accumulated $\\partial E / \\partial w$.\n",
    "$$\n",
    "\\Delta w = - \\epsilon \\partial E / \\partial w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23241e8-8113-4c8b-91fd-34dc1b770290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(W, dE_dw, epsilon):\n",
    "    updated_W = [[w_ji - epsilon * dE_dw_ji for w_ji, dE_dw_ji in zip(w_row, dE_dw_row)] \n",
    "                 for w_row, dE_dw_row in zip(W, dE_dw)]\n",
    "\n",
    "    return updated_W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8c805-7d55-4066-aadd-b75a165aa771",
   "metadata": {},
   "source": [
    "# Equation (9)\n",
    "The equation(8) can be significantly improved, without sacrificing the simplicity and locality,<br> by using an acceleration method in which the current gradient is used to modify the velocity of the point in weight space instead of its position.\n",
    "$$\n",
    "\\Delta w(t) = - \\epsilon \\partial E / \\partial w(t) + \\alpha \\Delta w (t-1)\n",
    "$$\n",
    "$where$<br>$t$ is incremented by 1 for each sweep through the whole set of input-output cases,<br>\n",
    "$\\alpha$ is an exponential decay factor between 0 and 1 that determines the relative contribution of the current gradient and earlier gradients to the weight change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a247fa7-fcac-4e92-9fe0-31c422f32654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_with_momentum(W, dE_dw, prev_delta_W, epsilon, alpha):\n",
    "    updated_W = []\n",
    "    current_delta_W = []\n",
    "    for w_row, dE_dw_row, prev_delta_W_row in zip(W, dE_dw, prev_delta_W):\n",
    "        updated_w_row = []\n",
    "        current_delta_w_row = []\n",
    "        for w_ji, dE_dw_ji, prev_delta_w_ji in zip(w_row, dE_dw_row, prev_delta_W_row):\n",
    "            delta_w_ji = -epsilon * dE_dw_ji + alpha * prev_delta_w_ji\n",
    "            updated_w_row.append(w_ji + delta_w_ji)\n",
    "            current_delta_w_row.append(delta_w_ji)\n",
    "        updated_W.append(updated_w_row)\n",
    "        current_delta_W.append(current_delta_w_row)\n",
    "\n",
    "    return updated_W, current_delta_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71e34f-772f-4e67-bec1-2b9768bca9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
